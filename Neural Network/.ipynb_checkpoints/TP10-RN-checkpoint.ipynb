{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 10 : Les réseaux de neurones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import colors\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- Réalisation d'un RN\n",
    "\n",
    "On va utiliser cet exemple pour vérifier la réalisation\n",
    "![exemple](RNPA-exp.png)\n",
    "\n",
    "### I-1- Les fonctions d'activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.84104179, 0.84290453]), array([0.1336905 , 0.13241648]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO réaliser la dérivée de la fonction d'activation logistique\n",
    "def logistique_deriver(Z, A): \n",
    "    return A * (1 - A)\n",
    "\n",
    "# API\n",
    "class Activation(object): \n",
    "    # Calculer l'activation en se basant sur Z (la somme linéaire)\n",
    "    def activer(self, Z):\n",
    "        pass\n",
    "    # Calculer la dérivée en se basant sur Z et l'activation A\n",
    "    def deriver(self, Z, A):\n",
    "        pass\n",
    "    \n",
    "class Logistique(Activation):\n",
    "    def activer(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    def deriver(self, Z, A):\n",
    "        return logistique_deriver(Z, A)\n",
    "\n",
    "logistique = Logistique()\n",
    "\n",
    "# ============================================\n",
    "# ================= TEST =====================\n",
    "# ============================================\n",
    "\n",
    "z4_1 = np.array([1.666, 1.68])\n",
    "a4_1 = logistique.activer(z4_1)\n",
    "a4_1p = logistique.deriver(z4_1, a4_1)\n",
    "\n",
    "#Résultat : \n",
    "# (array([0.84104179, 0.84290453]), array([0.1336905 , 0.13241648]))\n",
    "a4_1, a4_1p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-2- Les fonction du coût\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.83258146, 0.17078832]), array([ 6.25      , -1.18623962]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO réaliser la dérivée de la fonction d'erreur BCE\n",
    "def BCE_deriver(H, Y):\n",
    "    return (H - Y) / (H - H ** 2)\n",
    "\n",
    "# API\n",
    "class Cout(object): \n",
    "    # Calculer l'activation en se basant sur Z (la somme linéaire)\n",
    "    def calculer(self, H, Y):\n",
    "        pass\n",
    "    # Calculer la dérivée en se basant sur Z et l'activation A\n",
    "    def deriver(self, H, Y):\n",
    "        pass\n",
    "\n",
    "class BCE(Cout):\n",
    "    def calculer(self, H, Y):\n",
    "        return - (Y * np.log(H) + (1-Y) * np.log(1-H))\n",
    "    def deriver(self, H, Y):\n",
    "        return BCE_deriver(H, Y)\n",
    "    \n",
    "bce = BCE()\n",
    "\n",
    "# ============================================\n",
    "# ================= TEST =====================\n",
    "# ============================================\n",
    "\n",
    "H = np.array([0.840 , 0.843])\n",
    "Y = np.array([0., 1.])\n",
    "J = bce.calculer(H, Y)\n",
    "DJ = bce.deriver(H, Y)\n",
    "\n",
    "#Résultat : \n",
    "# (array([1.83258146, 0.17078832]), array([ 6.25      , -1.18623962]))\n",
    "J, DJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-3- Le neurone\n",
    "\n",
    "La fonction qui met à jour les paramètres prend en entrée : \n",
    "- $W$ une liste des poids; un vecteur de taille $Lp$ (le nombre des neurones de la couche précédente)\n",
    "- $b$ le biais \n",
    "- $z$ la combinaison linéaire du neurone courant; un vecteur de taille $M$ (le nombre des échantillons)\n",
    "- $a$ l'activation du neurone courant; un vecteur de taille $M$  \n",
    "- $a\\_past$ les activations des neurones de la couche précédente; une matrice de taille est $(M * Lp)$\n",
    "- $delta\\_next$ le delta calculé dans la couche suivante; une matrice de taille $M * Ln$ ($Ln$ : le nombre des neurones dans la couche suivante)\n",
    "- $w\\_next$ les poids vers la couche suivante; un vecteur de taille $Ln$\n",
    "- $act$ c'est un object de type \"Activation\"; il fournit deux méthodes : \"act.activer\" et \"act.deriver\"\n",
    "- $alpha$ le pas de l'entraînement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.49375218, 0.2046736 ]),\n",
       " -0.30324311474187016,\n",
       " array([ 0.00696306, -0.00047683]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter la fonction de mise à jours des poids d'un neurone\n",
    "def neurone_maj(W, b, z, a, a_past, delta_next, w_next, act, alpha=1.):\n",
    "    delta = np.multiply(np.dot(delta_next,w_next), act.deriver(z,a))\n",
    "    #dW = np.mean(np.dot(delta, a_past),axis=0)\n",
    "    #db = np.mean(delta,axis=0)\n",
    "    m = len(z)\n",
    "    dW = (1/m) * np.dot(delta, a_past)\n",
    "    db = (1/m) * np.sum(delta)\n",
    "    W = W - alpha * dW\n",
    "    b = b - alpha * db\n",
    "    return W, b, delta\n",
    "\n",
    "# ============================================\n",
    "# ================= TEST =====================\n",
    "# ============================================\n",
    "\n",
    "W = np.array([0.5, 0.2])\n",
    "b = -0.3\n",
    "z = np.array([0.5, 2.2])\n",
    "# M (l'activation actuelle)\n",
    "a = np.array([0.62245933, 0.90024951])\n",
    "# M * L (les activations de la couche précédente)\n",
    "a_past = np.array([[2., -1.], [3., 5.]])\n",
    "# L\n",
    "delta_next = np.array([[ 0.14523862, -0.02613822], [ 0.1394202, -0.02531591]]).T\n",
    "w_next = np.array([0.3, -0.1])\n",
    "act = logistique #la fonction d'activation\n",
    "\n",
    "\n",
    "W_nouv, b_nouv, delta_nouv = neurone_maj(W, b, z, a, a_past, delta_next, w_next, act, alpha=1.)\n",
    "\n",
    "# Résultat \n",
    "# (array([0.49375218, 0.2046736 ]),\n",
    "#  -0.30324311474187016,\n",
    "#  array([ 0.00696306, -0.00047683]))\n",
    "W_nouv, b_nouv, delta_nouv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z2_1 = [0.5 2.2]\n",
      "a2_1 = [0.62245933 0.90024951]\n",
      "derivee(a2_1) = [0.23500371 0.08980033]\n",
      "ancien b = -0.3\n",
      "ancien w = [0.5 0.2]\n",
      "delta2 = [ 0.00696306 -0.00047683]\n",
      "nouveaux b = -0.30324311473938026\n",
      "nouveaux w = [0.49375218 0.2046736 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Neurone(object):\n",
    "    def __init__(self, taille_entree, activation=logistique):\n",
    "        self.b = 0.\n",
    "        self.w = np.array([0.] * taille_entree)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def randomiser(self):\n",
    "        self.w = np.random.rand(len(self.w))\n",
    "        self.b = np.random.rand(1)[0]\n",
    "        \n",
    "    def __aggreger(self, X):\n",
    "        return np.dot(X, self.w) + self.b\n",
    "    \n",
    "    def activer(self, X):\n",
    "        self.a_past = X\n",
    "        self.z = self.__aggreger(X)\n",
    "        self.a = self.activation.activer(self.z)\n",
    "        return self.a\n",
    "    \n",
    "    def actualiser(self, delta_next, w_next, alpha=1.):\n",
    "        w_ancien = self.w.copy()\n",
    "        self.w, self.b, delta = neurone_maj(self.w, self.b, self.z, self.a, self.a_past, \n",
    "                                            delta_next, w_next, self.activation, alpha=alpha)\n",
    "        return delta, w_ancien\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# ================= TEST =====================\n",
    "# ============================================\n",
    "\n",
    "# M X L\n",
    "a1 = np.array([[2., -1.], [3., 5.]])\n",
    "# L\n",
    "delta3 = np.array([[ 0.14523862, -0.02613822], [ 0.1394202, -0.02531591]]).T\n",
    "w3_1 = np.array([0.3, -0.1])\n",
    "\n",
    "n = Neurone(2)\n",
    "#On ne doit pas affecter les poids directement \n",
    "#Ici, c'est juste pour avoir les mêmes poids du neurone de sortie dans l'exemple du cours\n",
    "n.b = -0.3\n",
    "n.w = np.array([0.5, 0.2])\n",
    "\n",
    "\n",
    "a2_1 = n.activer(a1)\n",
    "print(\"z2_1 = \" + str(n.z))\n",
    "print(\"a2_1 = \" + str(a2_1))\n",
    "\n",
    "# la dérivée de la fonction logistique n'a pas besoin de z, donc on passe 0\n",
    "print(\"derivee(a2_1) = \" + str(logistique.deriver(0,a2_1)))\n",
    "\n",
    "print(\"ancien b = \" + str(n.b))\n",
    "\n",
    "delta2, w2_ancien = n.actualiser(delta3, w3_1) \n",
    "\n",
    "print(\"ancien w = \" + str(w2_ancien))\n",
    "\n",
    "print(\"delta2 = \" + str(delta2))\n",
    "\n",
    "print(\"nouveaux b = \" + str(n.b))\n",
    "print(\"nouveaux w = \" + str(n.w))\n",
    "\n",
    "# Résultat \n",
    "# z2_1 = [0.5 2.2]\n",
    "# a2_1 = [0.62245933 0.90024951]\n",
    "# derivee(a2_1) = [0.23500371 0.08980033]\n",
    "# ancien b = -0.3\n",
    "# ancien w = [0.5 0.2]\n",
    "# delta2 = [ 0.00696306 -0.00047683]\n",
    "# nouveaux b = -0.30324311473938026\n",
    "# nouveaux w = [0.49375218 0.2046736 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-4- La couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activations : [[0.62245933 0.66818777]\n",
      " [0.90024951 0.96770454]]\n",
      "deltas : [[ 0.00696306  0.00682726]\n",
      " [-0.00047683 -0.00017109]]\n"
     ]
    }
   ],
   "source": [
    "class Couche(object):\n",
    "    \n",
    "    def __init__(self, taille, taille_entree, activation=logistique):\n",
    "        self.neurones = [Neurone(taille_entree, activation=activation) for i in range(taille)]\n",
    "        \n",
    "    def randomiser(self):\n",
    "        for neurone in self.neurones:\n",
    "            neurone.randomiser()\n",
    "\n",
    "    def propagation_avant(self, X):\n",
    "        activations = []\n",
    "        for neurone in self.neurones:\n",
    "            activations.append(neurone.activer(X))\n",
    "        return np.array(activations).T\n",
    "    \n",
    "    def retro_propagation(self, delta_next, W_next, alpha=1.):\n",
    "        W_anciens = []\n",
    "        Deltas = []\n",
    "        for i, neurone in enumerate(self.neurones):\n",
    "            delta, w_ancien = neurone.actualiser(delta_next, W_next[i], alpha=alpha)\n",
    "            W_anciens.append(w_ancien)\n",
    "            Deltas.append(delta)\n",
    "        return np.array(Deltas).T, np.array(W_anciens).T\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# ================= TEST =====================\n",
    "# ============================================\n",
    "\n",
    "a2 = np.array([[2., -1.], [3., 5.]])\n",
    "# L\n",
    "delta3 = np.array([[ 0.14523862, -0.02613822], [ 0.1394202, -0.02531591]]).T\n",
    "w3 = np.array([[0.3, -0.1],[0.5, -0.3]])\n",
    "\n",
    "c2 = Couche(2, 2)\n",
    "#On ne doit pas affecter les poids directement \n",
    "#Ici, c'est juste pour avoir les mêmes poids du neurone de sortie dans l'exemple du cours\n",
    "c2.neurones[0].b = -0.3\n",
    "c2.neurones[0].w = np.array([0.5, 0.2])\n",
    "c2.neurones[1].b = 0.5\n",
    "c2.neurones[1].w = np.array([0.3, 0.4])\n",
    "\n",
    "a2 = c2.propagation_avant(a1)\n",
    "print(\"activations : \" + str(a2))\n",
    "\n",
    "Deltas2, W_anciens2 = c2.retro_propagation(delta3, w3)\n",
    "\n",
    "print(\"deltas : \" + str(Deltas2))\n",
    "\n",
    "# Résultat : \n",
    "# activations : [[0.62245933 0.66818777]\n",
    "#  [0.90024951 0.96770454]]\n",
    "# deltas : [[ 0.00696306  0.00682726]\n",
    "#  [-0.00047683 -0.00017109]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-5- Le réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le cout = 1.0020916974430962\n",
      "w4_1 = [0.51494626 0.56592079]\n",
      "w3_1 = [0.2665629 0.4641237]\n",
      "w3_2 = [-0.13199638 -0.33433028]\n",
      "w2_1 = [0.49375219 0.2046736 ]\n",
      "w2_2 = [0.29342937 0.40384135]\n",
      "la prédiction : [0 1]\n"
     ]
    }
   ],
   "source": [
    "class RN(object):\n",
    "    def __init__(self, taille_entree, cout=bce, alpha=1.):\n",
    "        self.taille_courante = taille_entree #la taille de la dernière couche\n",
    "        self.cout = cout #objet de type Cout pour calculer le cout et sa dérivée\n",
    "        self.alpha = alpha\n",
    "        self.couches = []\n",
    "\n",
    "    def ajouter_couche(self, taille, activation=logistique):\n",
    "        nouv_couche = Couche(taille, self.taille_courante, activation=activation)\n",
    "        self.couches.append(nouv_couche)\n",
    "        self.taille_courante = taille\n",
    "        \n",
    "    def randomiser(self):\n",
    "        for couche in self.couches:\n",
    "            couche.randomiser()\n",
    "    \n",
    "    def predire(self, X): \n",
    "        Y = X\n",
    "        if self.norm:\n",
    "            Y = np.where(self.std==0, X, (X - self.mean)/self.std)\n",
    "            \n",
    "        for couche in self.couches:\n",
    "            Y = couche.propagation_avant(Y)\n",
    "        if Y.ndim == 2 and Y.shape[1] == 1:\n",
    "            Y = Y.flatten()\n",
    "        return np.where(Y < 0.5, 0, 1)\n",
    "    \n",
    "    \n",
    "    def _faire_iteration(self, X, Y):\n",
    "        # propagation avant\n",
    "        a = X\n",
    "        for couche in self.couches:\n",
    "            a = couche.propagation_avant(a)\n",
    "            \n",
    "        # calcul du cout et sa dérivée \n",
    "        YY = np.array(Y)\n",
    "        if YY.ndim < 2 : \n",
    "            YY = YY[:, np.newaxis]\n",
    "        J = np.mean(self.cout.calculer(a, YY))\n",
    "        J_prime = self.cout.deriver(a, YY)\n",
    "        \n",
    "        # retropropagation \n",
    "        w_past = np.array([[1.] * self.taille_courante])\n",
    "        delta_past = J_prime\n",
    "#         l = 5\n",
    "        for couche in reversed(self.couches): # on commance de la dernière couche vers la première\n",
    "#             print(l)\n",
    "#             print(delta_past)\n",
    "#             print(w_past)\n",
    "#             l = l - 1\n",
    "            delta_past, w_past = couche.retro_propagation(delta_past, w_past)\n",
    "        return J\n",
    "    \n",
    "    def entrainer(self, X, Y, nbr_it=100, norm=False):\n",
    "        couts = []\n",
    "        X_norm = X\n",
    "        self.norm = norm\n",
    "        if norm:\n",
    "            self.mean = np.mean(X, axis=0)\n",
    "            self.std = np.std(X, axis=0)\n",
    "            X_norm = np.where(self.std==0, X, (X - self.mean)/self.std)\n",
    "\n",
    "        for i in range(nbr_it): \n",
    "            J = self._faire_iteration(X_norm, Y)\n",
    "            couts.append(J)\n",
    "        return couts\n",
    "    \n",
    "# ============================================\n",
    "# ================= TEST =====================\n",
    "# ============================================\n",
    "\n",
    "X = np.array([[2., -1.], [3., 5.]])\n",
    "Y = np.array([0., 1.])\n",
    "\n",
    "rn = RN(2) #deux caractéristiques d'entrée\n",
    "rn.ajouter_couche(2) #ajouter une couche avec 2 neurones (cachée)\n",
    "rn.ajouter_couche(2) #ajouter une couche avec 2 neurones (cachée)\n",
    "rn.ajouter_couche(1) #ajouter une couche avec 1 neurone (sortie)\n",
    "\n",
    "#On ne doit pas affecter les poids directement \n",
    "#Ici, c'est juste pour avoir les mêmes poids du neurone de sortie dans l'exemple du cours\n",
    "rn.couches[0].neurones[0].b = -0.3\n",
    "rn.couches[0].neurones[0].w = np.array([0.5, 0.2])\n",
    "rn.couches[0].neurones[1].b = 0.5\n",
    "rn.couches[0].neurones[1].w = np.array([0.3, 0.4])\n",
    "\n",
    "rn.couches[1].neurones[0].b = -0.3\n",
    "rn.couches[1].neurones[0].w = np.array([0.3, 0.5])\n",
    "rn.couches[1].neurones[1].b = -0.2\n",
    "rn.couches[1].neurones[1].w = np.array([-0.1, -0.3])\n",
    "\n",
    "rn.couches[2].neurones[0].b = 1.\n",
    "rn.couches[2].neurones[0].w = np.array([0.7, 0.7])\n",
    "\n",
    "J = rn._faire_iteration(X, Y)\n",
    "\n",
    "print(\"le cout = \" + str(J))\n",
    "print(\"w4_1 = \" + str(rn.couches[2].neurones[0].w))\n",
    "print(\"w3_1 = \" + str(rn.couches[1].neurones[0].w))\n",
    "print(\"w3_2 = \" + str(rn.couches[1].neurones[1].w))\n",
    "print(\"w2_1 = \" + str(rn.couches[0].neurones[0].w))\n",
    "print(\"w2_2 = \" + str(rn.couches[0].neurones[1].w))\n",
    "\n",
    "rn.entrainer(X, Y, nbr_it=200)\n",
    "print(\"la prédiction : \" + str(rn.predire(X)))\n",
    "\n",
    "# Résultat \n",
    "# le cout = 1.0020916974430965\n",
    "# w4_1 = [0.51494626 0.56592079]\n",
    "# w3_1 = [0.2665629 0.4641237]\n",
    "# w3_2 = [-0.13199638 -0.33433028]\n",
    "# w2_1 = [0.49375219 0.2046736 ]\n",
    "# w2_2 = [0.29342937 0.40384135]\n",
    "# la prédiction : [0 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-6- Application sur une exemple\n",
    "\n",
    "On va utiliser le dataset [Diabetics prediction using logistic regression](https://www.kaggle.com/kandij/diabetes-dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diabetes2\n",
    "diabetes = pd.read_csv(\"datasets/diabetes2.csv\") \n",
    "X_diabetes = diabetes.iloc[:, :-1].values  \n",
    "Y_diabetes = diabetes.iloc[:, -1].values\n",
    "\n",
    "# Cette configuration est mise en place comme ceci exprès\n",
    "# C'est pour tester le cas où la régression est difavorisée\n",
    "NBR_TEST = 240\n",
    "# Supposant que les 30% premières lignes sont pour le test et le reste pour l'entraînement\n",
    "X_test = X_diabetes[-NBR_TEST:, :] # 30% ou plus\n",
    "Y_test = Y_diabetes[-NBR_TEST:]\n",
    "\n",
    "X_train = X_diabetes[:-NBR_TEST, :] \n",
    "Y_train = Y_diabetes[:-NBR_TEST]\n",
    "\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I-6-1- Randomisation des paramètres (thétas)\n",
    "\n",
    "On va créer 4 modèles :\n",
    "1. deux couches cachées avec 4 et 2 neurones respectivement et des paramètres mis à zéro\n",
    "1. le même  que le précédent mais on affecte des valeurs aléatoires aux paramètres\n",
    "1. une seule couche cachée avec 4 neurones et des paramètres mis à zéro\n",
    "\n",
    "**Analyse**\n",
    "\n",
    "On remarque que la fonction du coût ne change pas sa valeur lorsqu'on met les thétas à zéro, ce qui indique que les paramètres nne sont pas mis à jour. Pourquoi?\n",
    "Pourtant, lorsqu'il y a une seule couche cachée, les paramètres sont bien mis à jour.\n",
    "\n",
    "**Réponse**\n",
    "\n",
    "En fait, toute initialisation constante fonctionnera très mal. Considérons un réseau de neurones avec deux unités cachées et supposons que nous initialisons tous les $b$ à 0 et les $W$ avec une constante $\\alpha$. Si nous transmettons une entrée $(x_{1}, x_{2})$ dans ce réseau, la sortie des deux unités cachées sera $activation\\_function(\\alpha x_{1},\\alpha x_{2})$ Ainsi, les deux unités cachées auront une influence identique sur le coût, ce qui conduira à des gradients identiques. Ainsi, les deux neurones évolueront symétriquement tout au long de l'entrainement, empêchant efficacement différents neurones d'apprendre des choses différentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "IT = 300\n",
    "\n",
    "rn_norand = RN(8, alpha=0.1)\n",
    "rn_norand.ajouter_couche(4) #ajouter une couche avec 4 neurones (cachée)\n",
    "rn_norand.ajouter_couche(2) #ajouter une couche avec 2 neurones (cachée)\n",
    "rn_norand.ajouter_couche(1) #ajouter une couche avec 1 neurone (sortie)\n",
    "\n",
    "couts_norand = rn_norand.entrainer(X_train, Y_train, norm=True, nbr_it=IT)\n",
    "\n",
    "rn_rand = RN(8, alpha=0.1)\n",
    "rn_rand.ajouter_couche(4) #ajouter une couche avec 4 neurones (cachée)\n",
    "rn_rand.ajouter_couche(2) #ajouter une couche avec 2 neurones (cachée)\n",
    "rn_rand.ajouter_couche(1) #ajouter une couche avec 1 neurone (sortie)\n",
    "rn_rand.randomiser()\n",
    "\n",
    "couts_rand = rn_rand.entrainer(X_train, Y_train, norm=True, nbr_it=IT)\n",
    "\n",
    "rn_une = RN(8, alpha=0.1)\n",
    "rn_une.ajouter_couche(4) #ajouter une couche avec 4 neurones (cachée)\n",
    "rn_une.ajouter_couche(1) #ajouter une couche avec 1 neurone (sortie)\n",
    "couts_une= rn_une.entrainer(X_train, Y_train, norm=True, nbr_it=IT)\n",
    "\n",
    "plt.plot(couts_rand, color=\"red\", label=\"Theta aleatoires\")\n",
    "plt.plot(couts_norand, color=\"blue\", label=\"Theta = 0\")\n",
    "plt.plot(couts_une, color=\"green\", label=\"Une couche cachee\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I-6-2- Complexité du problème \n",
    "\n",
    "On veut tester l'impact du nombre des couches (et des neurones) sur la  convergence et le temps d'entraînement. \n",
    "Pour ce faire, on a proposer 4 architectures avec une couche de sortie d'un seul neurone : \n",
    "1. 3 couches cachées : 8 X 4 X 2 neurones\n",
    "1. 2 couches cachées : 4 X 2 neurones\n",
    "1. 1 couche cachée : 2 neurones\n",
    "1. 0 couches cachées : pratiquement c'est de la  régression logistique\n",
    "\n",
    "**Analyse**\n",
    "\n",
    "- Que remarquez-vous ?\n",
    "- Au delà de 2 couches (cas 3 couches), on constate que le modèle a arrêté à converger, comme le cas des paramètres initialisés à zéro. Pourquoi à votre avis ?\n",
    "- Est-ce que ce problème est aussi complexe qu'on aura besoin de plusieurs couches cachées ?\n",
    "- Dans ce problème, en comparant entre les deux architectures : 2 couches cachhées et la régression logistique, quelle est la meilleure et pourquoi ?\n",
    "\n",
    "**Réponse**\n",
    "\n",
    "- On remarque que la regression logistique converge plus rapidement que les réseaux de neuronnes d'une et deux couches, le réseau de neuronne de 3 couches s'arrete de converger plus rapidement que les autres.\n",
    "- les couches supplémentaires (au dela de 2)ont une influence identique sur le coût, ce qui conduira à des gradients identiques. Ainsi, les neurones évolueront symétriquement tout au long de l'entrainement, empêchant efficacement différents neurones d'apprendre des choses différentes.\n",
    "- Non, le probleme n'est pas aussi complexe au point où une regression logistique est meilleure que les réseaux de neuronnes.\n",
    "- La regression logistique est meileure pour ce problème vu qu'elle minimise le cout mieux que le réseau de neuronne à 2 couches et plus rapidement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps 3 couches cachees : 1.16985885400004\n",
      "temps 2 couches cachees : 0.5615222520000316\n",
      "temps 1 couche cachee : 0.3272252250001202\n",
      "temps regression logistique : 0.16260394599999017\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "IT = 400\n",
    "\n",
    "rn1 = RN(8, alpha=0.1)\n",
    "rn1.ajouter_couche(8) #ajouter une couche avec 4 neurones (cachée)\n",
    "rn1.ajouter_couche(4) #ajouter une couche avec 4 neurones (cachée)\n",
    "rn1.ajouter_couche(2) #ajouter une couche avec 2 neurones (cachée)\n",
    "rn1.ajouter_couche(1) #ajouter une couche avec 1 neurone (sortie)\n",
    "rn1.randomiser()\n",
    "\n",
    "rn2 = RN(8, alpha=0.1)\n",
    "rn2.ajouter_couche(4) #ajouter une couche avec 4 neurones (cachée)\n",
    "rn2.ajouter_couche(2) #ajouter une couche avec 2 neurones (cachée)\n",
    "rn2.ajouter_couche(1) #ajouter une couche avec 1 neurone (sortie)\n",
    "rn2.randomiser()\n",
    "\n",
    "rn3 = RN(8, alpha=0.1)\n",
    "rn3.ajouter_couche(2) #ajouter une couche avec 2 neurones (cachée)\n",
    "rn3.ajouter_couche(1) #ajouter une couche avec 1 neurone (sortie)\n",
    "rn3.randomiser()\n",
    "\n",
    "rn4 = RN(8, alpha=0.1)\n",
    "rn4.ajouter_couche(1) #ajouter une couche avec 1 neurone (sortie)\n",
    "rn4.randomiser()\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "couts1 = rn1.entrainer(X_train, Y_train, norm=True, nbr_it=IT)\n",
    "temps1 = timeit.default_timer() - temps_debut\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "couts2 = rn2.entrainer(X_train, Y_train, norm=True, nbr_it=IT)\n",
    "temps2 = timeit.default_timer() - temps_debut\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "couts3 = rn3.entrainer(X_train, Y_train, norm=True, nbr_it=IT)\n",
    "temps3 = timeit.default_timer() - temps_debut\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "couts4 = rn4.entrainer(X_train, Y_train, norm=True, nbr_it=IT)\n",
    "temps4 = timeit.default_timer() - temps_debut\n",
    "\n",
    "print(\"temps 3 couches cachees : \" + str(temps1))\n",
    "print(\"temps 2 couches cachees : \" + str(temps2))\n",
    "print(\"temps 1 couche cachee : \" + str(temps3))\n",
    "print(\"temps regression logistique : \" + str(temps4))\n",
    "\n",
    "plt.plot(couts1, color=\"red\", label=\"3 couches cachees\")\n",
    "plt.plot(couts2, color=\"green\", label=\"2 couches cachees\")\n",
    "plt.plot(couts3, color=\"blue\", label=\"1 couche cachee\")\n",
    "plt.plot(couts4, color=\"magenta\", label=\"regression logistique\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I-6-3- Performance\n",
    "\n",
    "Ici, on veut tester les modèles en se basant sur la métrique \"accuracy\" et le temps de prédiction. \n",
    "\n",
    "**Analyse**\n",
    "\n",
    "- Que remarquez-vous ?\n",
    "- Pourquoi en augmentant le nombre des couches, le modèle prend plus de temps à prédire le résultat ?\n",
    "\n",
    "**Réponse**\n",
    "\n",
    "- On remarque que pour ce problème la regression logistique présente la meilleure accurence en un temps minimal et aussi lorsque le nombre de couches du réseau de neuronne augmente: l'accuracy diminue et le temps de prédiction augmente.\n",
    "- A chaque couche qu'on ajoute à notre réseaux de neuronne, il y aura des opération en plus à calculer lors de la prédiction : On calcule d'abord le $Z$ en utilisant le $W$ et le $b$ de cette couche, et ensuite on lui applique la fonction d'activation pour avoir $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temps</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>architecture</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3 couches cachees</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2 couches cachees</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.804167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1 couche cachee</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.795833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>regression logistique</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.804167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          temps  accuracy\n",
       "architecture                             \n",
       "3 couches cachees      0.001638  0.666667\n",
       "2 couches cachees      0.001019  0.804167\n",
       "1 couche cachee        0.000620  0.795833\n",
       "regression logistique  0.000461  0.804167"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "temps_exec = []\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "Y1 = rn1.predire(X_test)\n",
    "temps_exec.append(timeit.default_timer() - temps_debut)\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "Y2 = rn2.predire(X_test)\n",
    "temps_exec.append(timeit.default_timer() - temps_debut)\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "Y3 = rn3.predire(X_test)\n",
    "temps_exec.append(timeit.default_timer() - temps_debut)\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "Y4 = rn4.predire(X_test)\n",
    "temps_exec.append(timeit.default_timer() - temps_debut)\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "accuracy.append(accuracy_score(Y_test, Y1))\n",
    "accuracy.append(accuracy_score(Y_test, Y2))\n",
    "accuracy.append(accuracy_score(Y_test, Y3))\n",
    "accuracy.append(accuracy_score(Y_test, Y4))\n",
    "\n",
    "stat = pd.DataFrame({\n",
    "    \"architecture\": [\"3 couches cachees\", \"2 couches cachees\", \"1 couche cachee\", \"regression logistique\"],\n",
    "    \"temps\": temps_exec,\n",
    "    \"accuracy\": accuracy\n",
    "})\n",
    "\n",
    "stat = stat.set_index([\"architecture\"])\n",
    "\n",
    "stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- Amélioration d'un modèle\n",
    "\n",
    "On va utiliser le dataset MNIST. \n",
    "Ici, on a utiliser seulement 100 échantillons pour l'entraînement (10 pour chaque chiffre) et 30 pour la validation (3 pour chaque chiffre). \n",
    "Donc, le modèle ne sera pas puissant ; c'est juste pour monter quelques applications dans le traitement d'images.\n",
    "\n",
    "Le dataset contient des images de 28*28 pixels représentant des chiffres écrits manuellement. chaque pixel est un nombre entre 0 et 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6eb60fe45525>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAveragePooling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUpSampling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.layers import Input, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense, Dropout, UpSampling2D\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "\n",
    "# cette instruction va télécharger la totalité du dataset\n",
    "# mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "train = pd.read_csv(\"datasets/mnist_train.csv\")\n",
    "test = pd.read_csv(\"datasets/mnist_test.csv\")\n",
    "\n",
    "x_train = train.iloc[:, :-1].values # Premières colonnes \n",
    "y_train = train.iloc[:,-1].values # Dernière colonne \n",
    "x_test = test.iloc[:, :-1].values # Premières colonnes \n",
    "y_test = test.iloc[:,-1].values # Dernière colonne \n",
    "\n",
    "x_train = np.reshape(x_train, [-1, 28, 28, 1])\n",
    "x_test = np.reshape(x_test, [-1, 28, 28, 1])\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "y_train_onehot = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_onehot = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    plt.title(\"N: \" + str(y_train[i]))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-1- Classement des chiffres\n",
    "\n",
    "Ici, on essaye d'entrainer un modèle qui reconnait les chiffres de 0 à 9. \n",
    "Il y a deux modèles : \n",
    "- Dans le première cellule, c'est une modèle basique (\n",
    "- Dans la deuxième cellule, c'est un modèle identique au premier. Sauf, vous devez le changer en ajoutant des couches : Dropout, MaxPooling2D, etc. Le deuxième modèle doit être une amélioration. Vous devez discuter comment vous l'avez améliorer, c-à-d, si vous avez ajouter une autre couche, pourquoi ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "NBR_IT = 100\n",
    "epochs = range(NBR_IT)\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "detect_basic = Sequential()\n",
    "detect_basic.add(Dense(128, activation=\"relu\", input_shape=input_shape))\n",
    "detect_basic.add(Flatten())\n",
    "detect_basic.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "detect_basic.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Adadelta(),\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"entrainement ...\")\n",
    "results = detect_basic.fit(x_train, y_train_onehot, epochs=NBR_IT, validation_data=(x_test, y_test_onehot), verbose=0)\n",
    "# print(\"evaluation ...\")\n",
    "# score = detect_basic.evaluate(x_test, y_test_onehot, verbose=0)\n",
    "\n",
    "history = results.history\n",
    "couts_train = history[\"loss\"]\n",
    "couts_test = history[\"val_loss\"]\n",
    "accuracy_train = history[\"acc\"]\n",
    "accuracy_test = history[\"val_acc\"]\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))\n",
    "ax[0].plot(epochs, couts_train, color=\"orange\", label=\"cout entrainement\")\n",
    "ax[0].plot(epochs, couts_test, color=\"blue\", label=\"cout test\")\n",
    "ax[1].plot(epochs, accuracy_train, color=\"orange\", label=\"accuracy entrainement\")\n",
    "ax[1].plot(epochs, accuracy_test, color=\"blue\", label=\"accuracy test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO amélioer le modèle du classement de chiffres\n",
    "# =============== Modifier ici ===================\n",
    "detect_good = Sequential()\n",
    "detect_good.add(Dense(128, activation=\"relu\", input_shape=input_shape))\n",
    "detect_good.add(Flatten())\n",
    "detect_good.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "detect_good.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Adadelta(),\n",
    "    metrics=[\"accuracy\"])\n",
    "# =============== Fin modification ================\n",
    "\n",
    "\n",
    "print(\"entrainement ...\")\n",
    "results = detect_good.fit(x_train, y_train_onehot, epochs=NBR_IT, validation_data=(x_test, y_test_onehot), verbose=0)\n",
    "# print(\"evaluation ...\")\n",
    "# score = detect_basic.evaluate(x_test, y_test_onehot, verbose=0)\n",
    "\n",
    "history = results.history\n",
    "\n",
    "couts_train2 = history[\"loss\"]\n",
    "couts_test2 = history[\"val_loss\"]\n",
    "accuracy_train2 = history[\"acc\"]\n",
    "accuracy_test2 = history[\"val_acc\"]\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))\n",
    "ax[0].plot(epochs, couts_train, color=\"orange\", label=\"cout entrainement (basique)\")\n",
    "ax[0].plot(epochs, couts_test, color=\"blue\", label=\"cout test (basique)\")\n",
    "ax[0].plot(epochs, couts_train2, color=\"red\", label=\"cout entrainement (ameliore)\")\n",
    "ax[0].plot(epochs, couts_test2, color=\"green\", label=\"cout test (ameliore)\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(epochs, accuracy_train, color=\"orange\", label=\"accuracy entrainement (basique)\")\n",
    "ax[1].plot(epochs, accuracy_test, color=\"blue\", label=\"accuracy test (basique)\")\n",
    "ax[1].plot(epochs, accuracy_train2, color=\"red\", label=\"accuracy entrainement (ameliore)\")\n",
    "ax[1].plot(epochs, accuracy_test2, color=\"green\", label=\"accuracy test (ameliore)\")\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-2- Débruitage des images\n",
    "\n",
    "Ici, on va implémenter un autoencodeur qui a comme but de débruiter les images. \n",
    "Tout d'abord, on ajoute du bruit aux images. Ensuite, l'autoencodeur est entraîné sur les images bruitées en entrée et leurs équivalentes non bruitées en sortie. \n",
    "\n",
    "Un modèle de base est fourni. Vous devez modifier ce modèle pour avoir un autre meilleur. Ceci en ajoutant des couches, en supprimant des couches et en modifiant les paramètres comme le nombre des neurones, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les images bruitées\n",
    "noise_factor = 0.3\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(x_train_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-2-1- Modèle de base \n",
    "\n",
    "**Ne modifier pas ça**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NBR_IT_ae = 100\n",
    "epochs_ae = range(NBR_IT_ae)\n",
    "\n",
    "input_img = Input(shape=(28, 28, 1))\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (7, 7, 32)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "bruit_basic = Model(input_img, decoded)\n",
    "bruit_basic.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "\n",
    "print(\"entrainement (basic) ...\")\n",
    "results = bruit_basic.fit(x_train_noisy, x_train,\n",
    "                epochs=NBR_IT_ae,\n",
    "                validation_data=(x_test_noisy, x_test),\n",
    "                verbose=0\n",
    "                )\n",
    "x_decoded = bruit_basic.predict(x_test_noisy)\n",
    "history = results.history\n",
    "couts_train = history[\"loss\"]\n",
    "couts_test = history[\"val_loss\"]\n",
    "\n",
    "\n",
    "plt.plot(epochs, couts_train, color=\"orange\", label=\"cout entrainement (basique)\")\n",
    "plt.plot(epochs, couts_test, color=\"blue\", label=\"cout test (basique)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i in range(n):\n",
    "    # image original\n",
    "    ax = plt.subplot(3, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # image bruitée\n",
    "    ax = plt.subplot(3, n, i + 1 + n)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # image débruitée\n",
    "    ax = plt.subplot(3, n, i + 1 + 2 * n)\n",
    "    plt.imshow(x_decoded[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-2-2- Modèle amélioré \n",
    "\n",
    "**Modifier ici**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO améliorer le modèle du débruitage des images\n",
    "# =============== Modifier ici ===================\n",
    "\n",
    "input_img2 = Input(shape=(28, 28, 1))\n",
    "\n",
    "x2 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img2)\n",
    "x2 = MaxPooling2D((2, 2), padding='same')(x2)\n",
    "x2 = Conv2D(32, (3, 3), activation='relu', padding='same')(x2)\n",
    "\n",
    "encoded2 = MaxPooling2D((2, 2), padding='same')(x2)\n",
    "\n",
    "# at this point the representation is (7, 7, 32)\n",
    "\n",
    "x2 = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded2)\n",
    "x2 = UpSampling2D((2, 2))(x2)\n",
    "x2 = Conv2D(32, (3, 3), activation='relu', padding='same')(x2)\n",
    "x2 = UpSampling2D((2, 2))(x2)\n",
    "decoded2 = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x2)\n",
    "\n",
    "\n",
    "bruit_good = Model(input_img2, decoded2)\n",
    "bruit_good.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "# =============== Fin modification ================\n",
    "\n",
    "print(\"entrainement (ameliore) ...\")\n",
    "results = bruit_good.fit(x_train_noisy, x_train,\n",
    "                epochs=NBR_IT_ae,\n",
    "                validation_data=(x_test_noisy, x_test),\n",
    "                verbose=0\n",
    "                )\n",
    "x_decoded2 = bruit_good.predict(x_test_noisy)\n",
    "history = results.history\n",
    "couts_train2 = history[\"loss\"]\n",
    "couts_test2 = history[\"val_loss\"]\n",
    "\n",
    "plt.plot(epochs, couts_train, color=\"orange\", label=\"cout entrainement (basique)\")\n",
    "plt.plot(epochs, couts_test, color=\"blue\", label=\"cout test (basique)\")\n",
    "plt.plot(epochs, couts_train2, color=\"red\", label=\"cout entrainement (ameliore)\")\n",
    "plt.plot(epochs, couts_test2, color=\"green\", label=\"cout test (ameliore)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(n):\n",
    "    # image original\n",
    "    ax = plt.subplot(4, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title(\"originale\")\n",
    "    \n",
    "    # image bruitée\n",
    "    ax = plt.subplot(4, n, i + 1 + n)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title(\"bruitee\")\n",
    "\n",
    "    # image débruitée (basique)\n",
    "    ax = plt.subplot(4, n, i + 1 + 2 * n)\n",
    "    plt.imshow(x_decoded[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title(\"basique\")\n",
    "    \n",
    "    # image débruitée (amélioré)\n",
    "    ax = plt.subplot(4, n, i + 1 + 3 * n)\n",
    "    plt.imshow(x_decoded2[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title(\"ameliore\")\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

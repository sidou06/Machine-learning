{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 09 : Machine à vecteurs de support (SVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import colors\n",
    "from util import preparer, generer_zeros_1\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- SVM Linéaire (LSVM)\n",
    "\n",
    "\n",
    "### I-1- La prédiction \n",
    "\n",
    "Dans la régression logistique binaire, une classe est notée par $1$ et l'autre par $0$. \n",
    "Dans SVM, une classe est notée par $1$ et l'autre par $-1$.\n",
    "Pour prédire le résultat, on utilise une fonction $signe$ sur la combinaison linéaire des attributs. \n",
    "\n",
    "$$signe(z) = \\begin{cases}\n",
    "1 & \\text{si } z \\ge 0 \\\\\n",
    "0 & \\text{sinon }\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_svm(X, Theta): \n",
    "    return np.dot(X, Theta)\n",
    "\n",
    "# TODO réaliser cette fonction\n",
    "def signe(z):\n",
    "    return None\n",
    "\n",
    "X_t = np.array([[2., -10.], [1., -2.], [1.5, 3.]])\n",
    "Theta_t = np.array([0.5, 0.25])\n",
    "\n",
    "z = z_svm(X_t, Theta_t)\n",
    "\n",
    "#Résultat : (array([-1.5,  0. ,  1.5]), array([-1,  1,  1]))\n",
    "z, signe(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-2- Fonction du coût\n",
    "\n",
    "\n",
    "$$ J = \\frac{1}{M} \\left( \\frac{1}{2} \\sum\\limits_{j=0}^{N} \\theta_j^2 + C \\sum\\limits_{i=0}^{M} \\max(0, 1- y^{(i)} z^{(i)}) \\right)$$\n",
    "Où :\n",
    "- $M$ nombre des échantillons et $N$ nombre des attributs\n",
    "- $(i)$ veut dire le ième échantillon\n",
    "- $\\theta_j$ est le paramètre de l'attribut $j$ (peu-être référencé par $w_j$ dans d'autres ouvrages)\n",
    "- $z^{(i)} = \\theta_0 + \\sum_{j=1}^{N} \\theta_j x_j^{(i)}$\n",
    "- $C$ est un paramètre avec deux objectifs différents: maximiser la marge et minimiser le nombre d'erreurs sur les données d'entraînement. En modifiant le paramètre C, nous pouvons choisir de privilégier un objectif sur un autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compléter\n",
    "def J_svm(Z, Y, Theta, C=1.):\n",
    "    return None\n",
    "\n",
    "X_t = np.array([[2., -10.], [1., -2.], [1.5, 3.]])\n",
    "Theta_t = np.array([0.5, 0.25])\n",
    "Y_t = np.array([0., 1., 1.])\n",
    "\n",
    "#Résultat : 0.8229166666666666\n",
    "J_svm(z_svm(X_t, Theta_t), Y_t, Theta_t, C=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-3- Fonction des gradients\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\theta_j} = \n",
    "\\frac{1}{M} \\sum\\limits_{i=1}^M \\begin{cases}\n",
    "\\theta_j & \\text{si } y^{(i)} z^{(i)} \\ge 1\\\\\n",
    "\\theta_j - C x^{(i)}_j y^{(i)} & \\text{sinon}  \\\\\n",
    "\\end{cases}$$\n",
    "Où :\n",
    "- $M$ nombre des échantillons et $N$ nombre des attributs\n",
    "- $(i)$ veut dire le ième échantillon\n",
    "- $\\theta_j$ est le paramètre de l'attribut $j$ (peu-être référencé par $w_j$ dans d'autres ouvrages)\n",
    "- $z^{(i)} = \\theta_0 + \\sum_{j=1}^{N} \\theta_j x_j^{(i)}$\n",
    "- $C$ est un paramètre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_svm(X, Z, Y, Theta, C=1.):\n",
    "    Thetas = np.array([Theta] * len(Y))\n",
    "    Ys = np.array([Y] * len(Theta)).T\n",
    "    Test = np.array([Y*Z >= 1] * len(Theta)).T\n",
    "    return np.mean(np.where(Test, Thetas, Thetas - C * X * Ys), axis=0)\n",
    "\n",
    "\n",
    "X_t = np.array([[2., -10.], [1., -2.], [1.5, 3.]])\n",
    "Theta_t = np.array([0.5, 0.25])\n",
    "Y_t = np.array([-1., 1., 1.])\n",
    "\n",
    "# Résultat : array([0.16666667, 0.91666667])   \n",
    "gradient_svm(X_t, z_svm(X_t, Theta_t), Y_t, Theta_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-4- Descente des gradients et regroupement des fonctions\n",
    "\n",
    "Ici, en plus de la déscente des gradients (GD), on va introduire une autre méthode : Déscente stochastique des gradients (SGD). \n",
    "\n",
    "Dans cette méthode, la mise à jours des paramètres se fait pour chaque échantillon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Déscente des gradients\n",
    "def GD_svm(X, Y, norm=True, const=True, nbr_iter=100, alpha=0.1, C=1.): \n",
    "\n",
    "    X_pre, mean, std = preparer(X, norm, const)\n",
    "    Theta = generer_zeros_1(X_pre.shape[1])\n",
    "    \n",
    "    couts = []\n",
    "    \n",
    "    for i in range(nbr_iter):\n",
    "        Z = z_svm(X_pre, Theta)\n",
    "        Theta -= alpha * gradient_svm(X_pre, Z, Y, Theta, C=C)\n",
    "        couts.append(J_svm(Z, Y, Theta, C=C))\n",
    "    \n",
    "    return Theta, mean, std, couts\n",
    "\n",
    "# Déscente stochastique des gradients (toutes les échantillons)\n",
    "def SGD_svm(X, Y, norm=True, const=True, nbr_iter=100, alpha=0.1, C=1.): \n",
    "\n",
    "    X_pre, mean, std = preparer(X, norm, const)\n",
    "    Theta = generer_zeros_1(X_pre.shape[1])\n",
    "    \n",
    "    couts = []\n",
    "    \n",
    "    for i in range(nbr_iter):\n",
    "        np.random.shuffle(X_pre)\n",
    "        for j in range(len(X_pre)): \n",
    "            X_1 = np.array([X_pre[j, :]])\n",
    "            Y_1 = np.array([Y[j]])\n",
    "            Z_1 = z_svm(X_1, Theta)\n",
    "            Theta -= alpha * gradient_svm(X_1, Z_1, Y_1, Theta, C=C)\n",
    "        Z = z_svm(X_pre, Theta)\n",
    "        couts.append(J_svm(Z, Y, Theta, C=C))\n",
    "    \n",
    "    return Theta, mean, std, couts\n",
    "\n",
    "import random\n",
    "\n",
    "# Déscente stochastique des gradients (un échantillon par itération)\n",
    "def SGD1_svm(X, Y, norm=True, const=True, nbr_iter=100, alpha=0.1, C=1.): \n",
    "\n",
    "    X_pre, mean, std = preparer(X, norm, const)\n",
    "    Theta = generer_zeros_1(X_pre.shape[1])\n",
    "    M = len(X_pre)\n",
    "    \n",
    "    couts = []\n",
    "    \n",
    "    for i in range(nbr_iter):\n",
    "        # Mettre à jours les paramettres sur un seul échantillon\n",
    "        j = random.randint(0, M-1) \n",
    "        X_1 = np.array([X_pre[j, :]])\n",
    "        Y_1 = np.array([Y[j]])\n",
    "        Z_1 = z_svm(X_1, Theta)\n",
    "        Theta -= alpha * gradient_svm(X_1, Z_1, Y_1, Theta, C=C)\n",
    "        #calculer le coût pour tous les échantillons\n",
    "        Z = z_svm(X_pre, Theta)\n",
    "        couts.append(J_svm(Z, Y, Theta, C=C))\n",
    "    \n",
    "    return Theta, mean, std, couts\n",
    "\n",
    "X_t = np.array([[2., -10.], [1., -2.], [1.5, 3.]])\n",
    "Y_t = np.array([-1., 1., 1.])\n",
    "\n",
    "Theta, mean, std, couts = GD_svm(X_t, Y_t, C=10.)\n",
    "Theta1, mean1, std1, couts1 = SGD_svm(X_t, Y_t, C=10.)\n",
    "Theta2, mean2, std2, couts2 = SGD1_svm(X_t, Y_t, C=10.)\n",
    "Theta, Theta1, Theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(object):\n",
    "    \n",
    "    def __init__(self, nbr_iter=100, alpha=0.1, norm=True, const=True, C=1., solver=\"GD\"): \n",
    "        self.nbr_iter = nbr_iter\n",
    "        self.alpha = alpha\n",
    "        self.norm = norm\n",
    "        self.const = const\n",
    "        self.C = C\n",
    "        if solver == \"SGD\":\n",
    "            self.entrainer_svm = SGD_svm\n",
    "        elif solver == \"SGD1\":\n",
    "            self.entrainer_svm = SGD1_svm\n",
    "        else:\n",
    "            self.entrainer_svm = GD_svm\n",
    "                \n",
    "    \n",
    "    def entrainer(self, X, Y): \n",
    "        YY = np.array(Y)\n",
    "        #Transformer les Y == 0 vers Y == -1\n",
    "        YY[YY==0] = -1\n",
    "        #Choisir la fonction d'optimisation \n",
    "        \n",
    "            \n",
    "        self.Theta, self.mean, self.std, self.couts = self.entrainer_svm(X, YY, \n",
    "                                                                  nbr_iter=self.nbr_iter, \n",
    "                                                                  alpha=self.alpha,  \n",
    "                                                                  norm=self.norm, \n",
    "                                                                  const=self.const,\n",
    "                                                                  C = self.C)\n",
    "        \n",
    "    # La prédiction\n",
    "    # si prob=True elle rend un vecteur de probabilités\n",
    "    # sinon elle rend une vecteur de 1 et 0\n",
    "    def predire(self, X): \n",
    "        X_pre, mean, std = preparer(X, self.norm, self.const, mean=self.mean, std=self.std)\n",
    "        return signe(z_svm(X_pre, Theta))\n",
    "\n",
    "    \n",
    "X_t = np.array([[2., -10.], [1., -2.], [1.5, 3.]])\n",
    "Y_t = np.array([0, 1, 1])\n",
    "svm_t = SVM()\n",
    "svm_t.entrainer(X_t, Y_t)\n",
    "svm_t.predire(np.array([[2., 6.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-4- Application sur des classes linéairement séparables\n",
    "\n",
    "On va utiliser des notes générées automatiquement (le code est dans le dossier datasets)\n",
    "\n",
    "**Analyser les résultats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_csv(\"datasets/notes.csv\")\n",
    "X_notes = notes.iloc[:, :-1].values # Premières colonnes \n",
    "\n",
    "r = notes[\"Admis\"] == 1\n",
    "\n",
    "Y_notes = notes.iloc[:,-1].values # Dernière colonne \n",
    "plt.scatter(X_notes[r, 0], X_notes[r, 1], color=\"green\", label=\"Admis\")\n",
    "\n",
    "plt.scatter(X_notes[~r, 0], X_notes[~r, 1], color=\"red\", label=\"Non admis\")\n",
    "\n",
    "plt.xlabel(\"Note1\")\n",
    "plt.ylabel(\"Note2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBR_IT = 100\n",
    "ALPHA = 0.1\n",
    "Cs = [1., 10., 100.]\n",
    "colors = [\"green\", \"blue\", \"violet\"]\n",
    "from reg import RegLogistique\n",
    "from util import ligne_decision\n",
    "\n",
    "\n",
    "def dessigner(modele, ax, xx, color=\"r\", sep=False, label=\"\"):\n",
    "\n",
    "    theta0 = modele.Theta[0]\n",
    "    theta1 = modele.Theta[1]\n",
    "    theta2 = modele.Theta[2]\n",
    "    mean = modele.mean\n",
    "    std = modele.std\n",
    "    \n",
    "    a = -theta1 / theta2\n",
    "    yy = mean[1] - std[1] * (theta0 + theta1 * (xx - mean[0])/std[0]) / theta2\n",
    "    ax.plot(xx, yy, 'k-', color=color, label=label)\n",
    "    \n",
    "    if sep:\n",
    "        a = -theta1 / theta2\n",
    "        margin = 1 / np.sqrt(theta1 ** 2 + theta2 ** 2)\n",
    "        yy_down = yy - np.sqrt(1 + a ** 2) * margin\n",
    "        yy_up = yy + np.sqrt(1 + a ** 2) * margin\n",
    "        plt.plot(xx, yy_down, 'k--', color=color)\n",
    "        plt.plot(xx, yy_up, 'k--', color=color)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "xx = np.linspace(X_notes[:,0].min(), X_notes[:,0].max(), 100)\n",
    "\n",
    "ax.scatter(X_notes[r, 0], X_notes[r, 1], color=\"green\", label=\"Admis\")\n",
    "\n",
    "ax.scatter(X_notes[~r, 0], X_notes[~r, 1], color=\"red\", label=\"Non admis\")\n",
    "\n",
    "reglog = RegLogistique(nbr_iter=NBR_IT, alpha=ALPHA)\n",
    "reglog.entrainer(X_notes, Y_notes)\n",
    "\n",
    "dessigner(reglog, ax, xx, color=\"red\", label=\"Regression logistique\")\n",
    "\n",
    "\n",
    "for i, C in enumerate(Cs):\n",
    "    svm = SVM(nbr_iter=NBR_IT, alpha=ALPHA, C=C)\n",
    "    svm.entrainer(X_notes, Y_notes)\n",
    "    dessigner(svm, ax, xx, color=colors[i], sep=True, label=\"SVM \" + str(C))\n",
    "\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-7- Comparison entre les méthodes d'optimisation\n",
    "\n",
    "On veut comparer entre GD et SGD en terme de convergence des coûts et de temps d'entraînement. \n",
    "Pour ce faire, on compare entre ces méthodes : \n",
    "- GD : La descente des gradients \n",
    "- SGD : La descente stochastique des gradients (avec prise en charge de tous les échantillons)\n",
    "- SGD1 : La descente stochastique des gradients (avec prise en charge d'un seul échantillon par itération)\n",
    "\n",
    "**Analyser les résultats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "NBR_IT = 100\n",
    "ALPHA = 0.1\n",
    "C = 10.\n",
    "svm_gd = SVM(nbr_iter=NBR_IT, alpha=ALPHA, C=C)\n",
    "temps_debut = timeit.default_timer()\n",
    "svm_gd.entrainer(X_notes, Y_notes)\n",
    "temps_gd = timeit.default_timer() - temps_debut\n",
    "\n",
    "svm_sgd = SVM(nbr_iter=NBR_IT, alpha=ALPHA, C=C, solver=\"SGD\")\n",
    "temps_debut = timeit.default_timer()\n",
    "svm_sgd.entrainer(X_notes, Y_notes)\n",
    "temps_sgd = timeit.default_timer() - temps_debut\n",
    "\n",
    "svm_sgd1 = SVM(nbr_iter=NBR_IT, alpha=ALPHA, C=C, solver=\"SGD1\")\n",
    "temps_debut = timeit.default_timer()\n",
    "svm_sgd1.entrainer(X_notes, Y_notes)\n",
    "temps_sgd1 = timeit.default_timer() - temps_debut\n",
    "\n",
    "print(\"temps d'entrainement (GD vs. SGD vs. SGD1)\")\n",
    "temps_gd, temps_sgd, temps_sgd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence des couts \n",
    "plt.plot(svm_gd.couts, color=\"red\", label=\"Descente des gradients\")\n",
    "plt.plot(svm_sgd.couts, color=\"blue\", label=\"Descente stochastique des gradients (tous)\")\n",
    "plt.plot(svm_sgd1.couts, color=\"green\", label=\"Descente stochastique des gradients (un)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-6- Application sur des classes non linéairement séparables\n",
    "\n",
    "On va utiliser des notes générées automatiquement (le code est dans le dossier datasets)\n",
    "\n",
    "**Analyser les résultats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles \n",
    "  \n",
    "# generating data \n",
    "X_nl, Y_nl = make_circles(n_samples = 500, noise = 0.05, factor=0.3) \n",
    "  \n",
    "# visualizing data \n",
    "plt.scatter(X_nl[:, 0], X_nl[:, 1], c = Y_nl, marker = '.') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBR_IT_nl = 100\n",
    "ALPHA_nl = 0.1\n",
    "Cs_nl = [60., 100., 200.]\n",
    "colors_nl = [\"green\", \"blue\", \"violet\"]\n",
    "from reg import RegLogistique\n",
    "from util import ligne_decision\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "xx_nl = np.linspace(X_nl[:,0].min(), X_nl[:,0].max(), 100)\n",
    "\n",
    "ax.scatter(X_nl[:, 0], X_nl[:, 1], c = Y_nl, marker = '.') \n",
    "\n",
    "reglog_nl = RegLogistique(nbr_iter=NBR_IT_nl, alpha=ALPHA_nl)\n",
    "reglog_nl.entrainer(X_nl, Y_nl)\n",
    "\n",
    "dessigner(reglog_nl, ax, xx_nl, color=\"red\", label=\"Regression logistique\")\n",
    "\n",
    "\n",
    "for i, C in enumerate(Cs_nl):\n",
    "    svm = SVM(nbr_iter=NBR_IT_nl, alpha=ALPHA_nl, C=C)\n",
    "    svm.entrainer(X_nl, Y_nl)\n",
    "    dessigner(svm, ax, xx_nl, color=colors_nl[i], sep=True, label=\"SVM \" + str(C))\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- SVM non Linéaire\n",
    "\n",
    "Notre fonction de prédiction est formalisée comme : \n",
    "$$ f(X) = \\sum\\limits_{i=1}^{M} \\theta X^{(i)} + \\theta_0$$\n",
    "\n",
    "On peut formaliser $\\theta$ comme ([Hilary, A. Zisserman (2015), Lecture 3: SVM dual, kernels and regression](http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf)) : \n",
    "$$ \\theta = \\sum\\limits_{i=1}^{M} \\alpha_i X^{(i)} Y^{(i)}$$\n",
    "\n",
    "La formation duale du problème sera : \n",
    "$$\\max\\limits_{\\alpha_i \\ge 0} \\sum\\limits_{i=1}^{M} \\alpha_i - \\frac{1}{2} \\sum\\limits_{i=1}^{M} \\sum\\limits_{j=1}^{M} \\alpha_i \\alpha_j y^{(i)} y^{(j)} x^{(i)} x^{(j)}$$\n",
    "sous les contraintes : \n",
    "$$0 \\le \\alpha_i \\le C$$\n",
    "$$\\sum\\limits_{i=1}^{M} \\alpha_i y^{(i)} = 0$$\n",
    "\n",
    "### II-1- Noyau \n",
    "\n",
    "**Rien à programmer ou analyser ici**\n",
    "\n",
    "On note $K(x^{(i)}, x^{(j)}) = x^{(i)} x^{(j)}$ et on l'appele un noyau linéaire.\n",
    "\n",
    "L'idée du noyau est de chercher une séparation linéaire du problème dans un espace de dimension supérieure.\n",
    "Ceci par l'application d'une fonction non linéaire sur les vecteurs d'entrée $X$.\n",
    "\n",
    "Noyau gaussien (RBF kernel) :\n",
    "$$K(x_1, x_2) = exp{(-\\frac{||x_1 - x_2||^2}{2 \\sigma^2})} = exp{(-\\frac{\\sum_i (x_1^{(i)} - x_2^{(i)})^2}{2 \\sigma^2})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_lineaire(X1, X2):\n",
    "    return np.dot(X1, X2.T)\n",
    "\n",
    "def kernel_gaussien(X1, X2, sigma=1):\n",
    "    # Le cas de calcul du kernel entre deux échantillons (entraînement)\n",
    "    # résultat: un scalar\n",
    "    if np.ndim(X1) == 1 and np.ndim(X2) == 1:\n",
    "        result = np.exp(- (np.linalg.norm(X1 - X2, 2)) ** 2 / (2 * sigma ** 2))\n",
    "    # Le cas de calcul du kernel \n",
    "    elif (np.ndim(X1) > 1 and np.ndim(X2) == 1) or (np.ndim(X1) == 1 and np.ndim(X2) > 1):\n",
    "        result = np.exp(- (np.linalg.norm(X1 - X2, 2, axis=1) ** 2) / (2 * sigma ** 2))\n",
    "    elif np.ndim(X1) > 1 and np.ndim(X2) > 1:\n",
    "        result = np.exp(- (np.linalg.norm(X1[:, np.newaxis] - X2[np.newaxis, :], 2, axis=2) ** 2) / (2 * sigma ** 2))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-2- Fonction objectif\n",
    "\n",
    "$$W(\\alpha) = \\sum\\limits_{i=1}^{M} \\alpha_i - \\frac{1}{2} \\sum\\limits_{i=1}^{M} \\sum\\limits_{j=1}^{M} \\alpha_i \\alpha_j y^{(i)} y^{(j)} K(x^{(i)}, x^{(j)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implémenter\n",
    "def f_objectif(X, Y, alphas, K):\n",
    "    return None\n",
    "\n",
    "a_t = np.array([1., .5, .25])\n",
    "X_t = np.array([[1., 2.], [2., 3.], [2., 1.]])\n",
    "Y_t = np.array([1, -1, 1])\n",
    "\n",
    "# Résultat : 1.2026367706974372\n",
    "f_objectif(X_t, Y_t, a_t, kernel_gaussien)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-3- Fonction de décision \n",
    "\n",
    "$$\\hat{y_t} = f(x_t) = \\sum^M_{i=1} \\alpha_i y^{(i)} K(x^{(i)}, x_t) - b$$\n",
    "Où, \n",
    "- $x$ : les échantillons d'entraînement\n",
    "- $x_t$ : les échantillons de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implémenter\n",
    "def f_decision(X_test, X_train, Y_train, alphas, b, K):\n",
    "    return None\n",
    "\n",
    "a_t = np.array([1., .5, .25])\n",
    "X_t = np.array([[1., 2.], [2., 3.], [2., 1.]])\n",
    "Y_t = np.array([1, -1, 1])\n",
    "X_tt = np.array([[1., 1.], [2., 2.]])\n",
    "\n",
    "#Résultat : array([-0.28287917, -0.54510201])\n",
    "f_decision(X_tt, X_t, Y_t, a_t, 1., kernel_gaussien)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-4- Sequential minimal optimization\n",
    "\n",
    "**Rien à programmer ou analyser ici**\n",
    "\n",
    "Pour maximiser la fonction objectif, on va utiliser la méthode [Sequential minimal optimization)(https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf)\n",
    "\n",
    "Ce code est une adaptation de [Implementing a Support Vector Machine using Sequential Minimal Optimization and Python 3.5](https://jonchar.net/notebooks/SVM/#Sequential-minimal-optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executer_etape(X, Y, alphas, b, erreurs, K, C, i1, i2, tol, eps):\n",
    "    \n",
    "    # Si les alphas choisis sont les mêmes, sortir\n",
    "    if i1 == i2:\n",
    "        return False, b\n",
    "    \n",
    "    alpha1 = alphas[i1]\n",
    "    alpha2 = alphas[i2]\n",
    "    y1 = Y[i1]\n",
    "    y2 = Y[i2]\n",
    "    E1 = erreurs[i1]\n",
    "    E2 = erreurs[i2]\n",
    "    s = y1 * y2\n",
    "    \n",
    "    # Compute L & H, the bounds on new possible alpha values\n",
    "    if (y1 != y2):\n",
    "        L = max(0, alpha2 - alpha1)\n",
    "        H = min(C, C + alpha2 - alpha1)\n",
    "    else:\n",
    "        L = max(0, alpha1 + alpha2 - C)\n",
    "        H = min(C, alpha1 + alpha2)\n",
    "        \n",
    "    if (L == H):\n",
    "        return False, b\n",
    "\n",
    "    # Compute kernel & 2nd derivative eta\n",
    "    k11 = K(X[i1], X[i1])\n",
    "    k12 = K(X[i1], X[i2])\n",
    "    k22 = K(X[i2], X[i2])\n",
    "    eta = 2 * k12 - k11 - k22\n",
    "    \n",
    "    # Compute new alpha 2 (a2) if eta is negative\n",
    "    if (eta < 0):\n",
    "        a2 = alpha2 - y2 * (E1 - E2) / eta\n",
    "        # Clip a2 based on bounds L & H\n",
    "        if a2 < L:\n",
    "            a2 = L\n",
    "        elif a2 > H:\n",
    "            a2 = H\n",
    "            \n",
    "    # If eta is non-negative, move new a2 to bound with greater objective function value\n",
    "    else:\n",
    "        alphas_adj = alphas.copy()\n",
    "        alphas_adj[i2] = L\n",
    "        # objective function output with a2 = L\n",
    "        Lobj = f_objectif(X, Y, alphas_adj, K) \n",
    "        alphas_adj[i2] = H\n",
    "        # objective function output with a2 = H\n",
    "        Hobj = f_objectif(X, Y, alphas_adj, K)\n",
    "        if Lobj > (Hobj + eps):\n",
    "            a2 = L\n",
    "        elif Lobj < (Hobj - eps):\n",
    "            a2 = H\n",
    "        else:\n",
    "            a2 = alpha2\n",
    "            \n",
    "    # Push a2 to 0 or C if very close\n",
    "    if a2 < 1e-8:\n",
    "        a2 = 0.0\n",
    "    elif a2 > (C - 1e-8):\n",
    "        a2 = C\n",
    "    \n",
    "    # If examples can't be optimized within epsilon (eps), skip this pair\n",
    "    if np.abs(a2 - alpha2) < eps * (a2 + alpha2 + eps):\n",
    "        return False, b\n",
    "    \n",
    "    # Calculate new alpha 1 (a1)\n",
    "    a1 = alpha1 + s * (alpha2 - a2)\n",
    "    \n",
    "    # Update threshold b to reflect newly calculated alphas\n",
    "    # Calculate both possible thresholds\n",
    "    b1 = E1 + y1 * (a1 - alpha1) * k11 + y2 * (a2 - alpha2) * k12 + b\n",
    "    b2 = E2 + y1 * (a1 - alpha1) * k12 + y2 * (a2 - alpha2) * k22 + b\n",
    "    \n",
    "    # Set new threshold based on if a1 or a2 is bound by L and/or H\n",
    "    if 0 < a1 < C:\n",
    "        b_new = b1\n",
    "    elif 0 < a2 < C:\n",
    "        b_new = b2\n",
    "    # Average thresholds if both are bound\n",
    "    else:\n",
    "        b_new = (b1 + b2)/2.\n",
    "\n",
    "    # Update model object with new alphas & threshold\n",
    "    alphas[i1] = a1\n",
    "    alphas[i2] = a2\n",
    "    \n",
    "    # Update error cache\n",
    "    # Error cache for optimized alphas is set to 0 if they're unbound\n",
    "    for index, alph in zip([i1, i2], [a1, a2]):\n",
    "        if 0.0 < alph < C:\n",
    "            erreurs[index] = 0.0\n",
    "    \n",
    "    # Set non-optimized errors based on equation 12.11 in Platt's book\n",
    "    non_opt = [n for n in range(len(X)) if (n != i1 and n != i2)]\n",
    "    erreurs[non_opt] += y1 * (a1 - alpha1) * K(X[i1], X[non_opt])\n",
    "    erreurs[non_opt] += y2 * (a2 - alpha2) * K(X[i2], X[non_opt])\n",
    "    erreurs[non_opt] += b - b_new\n",
    "    \n",
    "    return True, b_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examiner_exemple(X, Y, alphas, b, erreurs, K, C, i2, tol, eps):\n",
    "    M = len(X)\n",
    "    y2 = Y[i2]\n",
    "    alpha2 = alphas[i2]\n",
    "    E2 = erreurs[i2]\n",
    "    r2 = E2 * y2\n",
    "\n",
    "    # Proceed if error is within specified tolerance (tol)\n",
    "    if ((r2 < -tol and alpha2 < C) or (r2 > tol and alpha2 > 0)):\n",
    "        indices_examiner2 = np.where((alphas != 0) & (alphas != C))[0]\n",
    "        M2 = len(indices_examiner2)\n",
    "        if M2 > 1:\n",
    "            # Use 2nd choice heuristic is choose max difference in error\n",
    "            if erreurs[i2] > 0:\n",
    "                i1 = np.argmin(erreurs)\n",
    "            else:\n",
    "                i1 = np.argmax(erreurs)\n",
    "                \n",
    "            step_result, b = executer_etape(X, Y, alphas, b, erreurs, K, C, i1, i2, tol, eps)\n",
    "            if step_result:\n",
    "                return True, b\n",
    "            \n",
    "            # Loop through non-zero and non-C alphas, starting at a random point\n",
    "            for i1 in np.roll(indices_examiner2, np.random.choice(np.arange(M2))):\n",
    "                step_result, b = executer_etape(X, Y, alphas, b, erreurs, K, C, i1, i2, tol, eps)\n",
    "                if step_result:\n",
    "                    return True, b\n",
    "        \n",
    "        # loop through all alphas, starting at a random point\n",
    "        for i1 in np.roll(np.arange(M), np.random.choice(np.arange(M))):\n",
    "            step_result, b = executer_etape(X, Y, alphas, b, erreurs, K, C, i1, i2, tol, eps)\n",
    "            if step_result:\n",
    "                return True, b\n",
    "    \n",
    "    return False, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ksvm_entrainer(X, Y, K, C=10., tol = 0.01, eps = 0.01):\n",
    "    X_pre, mean, std = preparer(X, norm=True, const=False)\n",
    "    M = len(X_pre)\n",
    "    alphas = np.zeros(M)\n",
    "    b = 0.0\n",
    "    numChanged = 0\n",
    "    examineAll = True\n",
    "    objs = []\n",
    "    erreurs = f_decision(X, X, Y, alphas, b, K) - Y\n",
    "    \n",
    "    while numChanged > 0 or examineAll:\n",
    "        numChanged = 0\n",
    "        if examineAll :\n",
    "            indices_examiner = range(M)\n",
    "        else: \n",
    "            indices_examiner = np.where((alphas != 0) & (alphas != C))[0]\n",
    "        \n",
    "        for i2 in indices_examiner:\n",
    "            examine_result, b = examiner_exemple(X, Y, alphas, b, erreurs, K, C, i2, tol, eps)\n",
    "            if examine_result:\n",
    "                numChanged += 1\n",
    "                objs.append(f_objectif(X, Y, alphas, K))\n",
    "                    \n",
    "        if examineAll == True:\n",
    "            examineAll = False\n",
    "        elif numChanged == 0:\n",
    "            examineAll = True\n",
    "        \n",
    "    return alphas, b, objs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-5- Regrouper les fonctions\n",
    "\n",
    "**Rien à programmer ou analyser ici**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSVM(object):\n",
    "    def __init__(self, K=kernel_lineaire, C=10., tol = 0.01, eps = 0.01):\n",
    "        self.C = C      # regularization parameter\n",
    "        self.K = K     # kernel function\n",
    "        self.tol = tol #tolérence des erreurs\n",
    "        self.eps = eps #tolérence des alphas\n",
    "    \n",
    "    def entrainer(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.alphas, self.b, self.objs = ksvm_entrainer(X, Y, self.K, self.C, tol = self.tol, eps = self.eps)\n",
    "    \n",
    "    def predire(self, X): \n",
    "        return f_decision(X, self.X, self.Y, self.alphas, self.b, self.K)\n",
    "    \n",
    "    def dessiner2D(self, ax, resolution=100, colors=('b', 'k', 'r'), levels=(-1, 0, 1)):\n",
    "        # Generate coordinate grid of shape [resolution x resolution]\n",
    "        # and evaluate the model over the entire space\n",
    "        xrange = np.linspace(self.X[:,0].min(), self.X[:,0].max(), resolution)\n",
    "        yrange = np.linspace(self.X[:,1].min(), self.X[:,1].max(), resolution)\n",
    "        grid = [[self.predire(np.array([xr, yr])) for xr in xrange] for yr in yrange]\n",
    "        grid = np.array(grid).reshape(len(xrange), len(yrange))\n",
    "        \n",
    "        # Plot decision contours using grid and\n",
    "        # make a scatter plot of training data\n",
    "        ax.contour(xrange, yrange, grid, levels=levels, linewidths=(1, 1, 1),\n",
    "                   linestyles=('--', '-', '--'), colors=colors)\n",
    "        ax.scatter(self.X[:,0], self.X[:,1],\n",
    "                   c=self.Y, cmap=plt.cm.viridis, lw=0, alpha=0.25)\n",
    "        \n",
    "        # Plot support vectors (non-zero alphas)\n",
    "        # as circled points (linewidth > 0)\n",
    "        mask = np.round(self.alphas, decimals=2) != 0.0\n",
    "        ax.scatter(self.X[mask,0], self.X[mask,1],\n",
    "                   c=self.Y[mask], cmap=plt.cm.viridis, lw=1, edgecolors='k')\n",
    "        \n",
    "        return grid, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-6- Application du kernel RBF sur des classes non linéairement séparables\n",
    "\n",
    "**Que remarquez-vous concernant la ligne de décision ?**\n",
    "\n",
    "**Que remarquez-vous concernant le nombre des itérations ?**\n",
    "\n",
    "**A votre avis, pourquoi l'algorithme n'a pas arrêté lorsque la fonction objectif a atteint le maximum ?** (ça dépond le dataset généré, ce cas peut ne pas se produire. Mais, la question est dans le cas général)\n",
    "\n",
    "PS. l'entraînement prend du temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_nl2 = Y_nl.copy()\n",
    "\n",
    "Y_nl2[Y_nl2 == 0] = -1\n",
    "\n",
    "ksvm = KSVM(C=1.0, K=kernel_gaussien)\n",
    "\n",
    "ksvm.entrainer(X_nl, Y_nl2)\n",
    "print(\"fin entrainement; plotting ...\")\n",
    "fig, ax = plt.subplots()\n",
    "grid, ax = ksvm.dessiner2D(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La fonction objectif (à maximiser)\n",
    "plt.plot(ksvm.objs, color=\"red\", label=\"Fonction objectif\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

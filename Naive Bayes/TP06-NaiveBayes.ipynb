{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP06 : Naïve Bayes\n",
    "\n",
    "Tout le monde connait le théorème de Bayes pour calculer la probabilité conditionnelle d'un évennement $A$ sachant un autre $B$: \n",
    "$$ P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "\n",
    "Pour appliquer ce théorème sur un problème d'appentissage automatique, l'idée est simple ; Etant donné une caractéristique $f$ et la sortie $y$ qui peut avoir la classe $c$ : \n",
    "- Remplacer $A$ par $y=c$\n",
    "- Remplacer $B$ par $f$ \n",
    "On aura l'équation : \n",
    "$$ P(y=c|f) = \\frac{P(y=c)P(f|y=c)}{P(f)}$$\n",
    "\n",
    "On appelle : \n",
    "- $P(y=c|f)$ postérieure \n",
    "- $P(y=c)$ antérieure\n",
    "- $P(f|y=c)$ vraisemblance\n",
    "- $P(f)$ évidence \n",
    "\n",
    "Ici, on estime la probablité d'une classe $c$ sachant une caractéristique $f$ en utilisant des données d'entrainement. Maintenant, on veut estimer la probabilité d'une classe $c$ sachant un vecteur de caractéristiques $\\overrightarrow{f} = \\{f_1, ..., f_L\\}$ : \n",
    "$$ P(y=c|\\overrightarrow{f}) = \\frac{P(y=c)P(\\overrightarrow{f}|y=c)}{P(f)}$$\n",
    "\n",
    "Etant donnée plusieurs classes $c_j$, la classe choisie $\\hat{c}$ est celle avec la probabilité maximale \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k|\\overrightarrow{f})$$\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\frac{P(y=c_k)P(\\overrightarrow{f}|y=c_k)}{P(f)}$$\n",
    "On supprime l'évidence pour cacher le crime : $P(f)$ ne dépend pas de $c_k$ et elle est postive, donc ça ne va pas affecter la fonction $\\max$.\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k)P(\\overrightarrow{f}|y=c_k)$$\n",
    "\n",
    "Pour calculer $P(\\overrightarrow{f}|y=c_k)$, on va utiliser une properiété naïve (d'où vient le nom Naive Bayes) : on suppose l'indépendence conditionnelle entre les caractéristiques $f_j$. \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k) \\prod\\limits_{f_j \\in \\overrightarrow{f}} P(f_j|y=c_k)$$\n",
    "\n",
    "Pour éviter la disparition de la probabilité (multiplication et représentation de virgule flottante sur machine), on transforme vers l'espace logarithme.\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j|y=c_k)$$\n",
    "\n",
    "\n",
    "## Avantages \n",
    "\n",
    "Les classifieurs naïfs bayésiens, malgré leur simplicité, ont des points forts:\n",
    "- Ils ont besoin d'une petite quantité de données d'entrainement.\n",
    "- Ils sont très rapides par rapport aux autres classifieurs.\n",
    "- Ils donnent de bons résultats dans le cas de filtrage du courrier indésirable et de classification de documents.\n",
    "\n",
    "## Limites\n",
    "Les classifieurs naïfs bayésiens certes sont populaires à cause de leur simplicité. Mais, une telle simplicité vient avec un coût [référence: Spiderman].\n",
    "- Les probabilités obtenues en utilisant ces classifieurs ne doivent pas être prises au sérieux.\n",
    "- S'il existe une grande corrélation entre les caractéristiques, ils vont donner une mauvaise performance.\n",
    "- Dans le cas des caractéristiques continues (prix, surface, etc.), les données doivent suivre la loi normale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- Implémentation\n",
    "\n",
    "Pour estimer la vraisemblance, il y a plusieurs modèles (lois):\n",
    "- Loi multinomiale : pour les caracétristiques nominales\n",
    "- Loi de Bernoulli : lorsqu'on est interressé par l'apparence d'une caractéristique ou non (binaire)\n",
    "- loi normale : pour les caractéristiques numériques\n",
    "\n",
    "Dans ce TP, on va implémenter Naive Bayes pour les caractéristiques nominales (loi multinomiale)\n",
    "\n",
    "### I-1- Les données pour les tests unitaires\n",
    "Ici, on va utiliser le dataset \"jouer\" contenant des caractéristiques nominales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temps</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidite</th>\n",
       "      <th>vent</th>\n",
       "      <th>jouer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>nuageux</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>nuageux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>douce</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>nuageux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>nuageux</td>\n",
       "      <td>chaude</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        temps temperature humidite vent jouer\n",
       "0   ensoleile      chaude    haute  non   non\n",
       "1   ensoleile      chaude    haute  oui   non\n",
       "2     nuageux      chaude    haute  non   oui\n",
       "3    pluvieux       douce    haute  non   oui\n",
       "4    pluvieux     fraiche  normale  non   oui\n",
       "5    pluvieux     fraiche  normale  oui   non\n",
       "6     nuageux     fraiche  normale  oui   oui\n",
       "7   ensoleile       douce    haute  non   non\n",
       "8   ensoleile     fraiche  normale  non   oui\n",
       "9    pluvieux       douce  normale  non   oui\n",
       "10  ensoleile       douce  normale  oui   oui\n",
       "11    nuageux       douce    haute  oui   oui\n",
       "12    nuageux      chaude  normale  non   oui\n",
       "13   pluvieux       douce    haute  oui   non"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "jouer = pd.read_csv(\"datasets/jouer.csv\")\n",
    "\n",
    "X_jouer = jouer.iloc[:, :-1].values # Premières colonnes \n",
    "Y_jouer = jouer.iloc[:,-1].values # Dernière colonne \n",
    "\n",
    "# Afficher le dataset \"jouer\"\n",
    "jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-2- Estimation de la probabilité antérieure\n",
    "Etant donné le vecteur de sortie $Y$, on doit calculer la probabilité de chaque classe (différentes valeurs de $Y$)\n",
    "\n",
    "$$p(c_k) = \\frac{|\\{y / y \\in Y \\text{ et } y = c_k\\}|}{|Y|}$$\n",
    "\n",
    "La fonction doit retourner un dictionnaire où la clé est le nom de la classe et la valeur est sa probabilité. Voici, un exemple d'un dictionnaire dans Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Iris-setosa)= 0.5\n",
      "P(Iris-versicolor)= 0.33\n",
      "P(Iris-virginica)= 0.67\n"
     ]
    }
   ],
   "source": [
    "# Exemple de dictionnaire dans Python \n",
    "d = {}\n",
    "d[\"Iris-setosa\"] = 0.5\n",
    "d[\"Iris-versicolor\"] = 0.33\n",
    "d[\"Iris-virginica\"] = 0.67\n",
    "\n",
    "for c in d: \n",
    "    print(\"P(\" + c + \")= \" + str(d[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'non': 0.35714285714285715, 'oui': 0.6428571428571429}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Réaliser la fonction \n",
    "def P_c(Y): \n",
    "    resultat = {}\n",
    "    valeurs = np.unique(Y)\n",
    "    for value in valeurs:\n",
    "        resultat[value] = len(Y[Y == value]) / len(Y)\n",
    "    return resultat\n",
    "\n",
    "# Résultat: {'non': 0.35714285714285715, 'oui': 0.6428571428571429}\n",
    "P_c(Y_jouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-3- Entrainement  (loi multinomiale)\n",
    "\n",
    "Notre modèle (notons le par $\\theta_{f_j,C}$) doit garder le nombre des différentes valeurs dans une caractéristique $A$ et le nombre de ces valeurs dans chaque classe.\n",
    "\n",
    "Donc, étant donné un vecteur d'une caractéristique $A$ et un autre des $Y$ respectives, la fonction d'entrainement doit retourner un dictionnaire (notre théta) : \n",
    "- la clé est une valeur $a_v$ de $A$ \n",
    "- la valeur est un autre dictionnaire : \n",
    "   - il doit contenir une clé \"_total_\" dont la valeur est le nombre d'occurence de $a_v$ dans $A$ \n",
    "   - la clé est la classe $c_k$ de $Y$\n",
    "   - la valeur est le nombre d'occurence de $a_v$ respectives à $c_k$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ensoleile': {'_total_': 5, 'non': 3, 'oui': 2},\n",
       " 'nuageux': {'_total_': 4, 'non': 0, 'oui': 4},\n",
       " 'pluvieux': {'_total_': 5, 'non': 2, 'oui': 3}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Réaliser cette fonction \n",
    "# Elle génère théta pour une seule caractéristique\n",
    "def entrainer_multi_1(A, Y): \n",
    "    resultat = {}\n",
    "    valeurs_A = np.unique(A)\n",
    "    valeurs_Y = np.unique(Y)\n",
    "    for value in valeurs_A:\n",
    "        li_A = {}\n",
    "        li_A[\"_total_\"] = len(A[A==value])\n",
    "        for value2 in valeurs_Y:\n",
    "            li_A[value2] = len(Y[(Y==value2) & (A==value)])\n",
    "        resultat[value] = li_A\n",
    "    return resultat\n",
    "\n",
    "# Résultat \n",
    "# {'ensoleile': {'_total_': 5, 'non': 3, 'oui': 2},\n",
    "# 'nuageux': {'_total_': 4, 'non': 0, 'oui': 4},\n",
    "# 'pluvieux': {'_total_': 5, 'non': 2, 'oui': 3}}\n",
    "Theta_jouer_temps = entrainer_multi_1(X_jouer[:, 0], Y_jouer)\n",
    "\n",
    "Theta_jouer_temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ensoleile': {'_total_': 5, 'non': 3, 'oui': 2},\n",
       "  'nuageux': {'_total_': 4, 'non': 0, 'oui': 4},\n",
       "  'pluvieux': {'_total_': 5, 'non': 2, 'oui': 3}},\n",
       " {'chaude': {'_total_': 4, 'non': 2, 'oui': 2},\n",
       "  'douce': {'_total_': 6, 'non': 2, 'oui': 4},\n",
       "  'fraiche': {'_total_': 4, 'non': 1, 'oui': 3}},\n",
       " {'haute': {'_total_': 7, 'non': 4, 'oui': 3},\n",
       "  'normale': {'_total_': 7, 'non': 1, 'oui': 6}},\n",
       " {'non': {'_total_': 8, 'non': 2, 'oui': 6},\n",
       "  'oui': {'_total_': 6, 'non': 3, 'oui': 3}},\n",
       " {'non': 0.35714285714285715, 'oui': 0.6428571428571429}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La fonction qui entraine Théta sur plusieurs caractéristiques\n",
    "# Rien à programmer ici\n",
    "# Notre théta est une liste des dictionnaires;\n",
    "# chaque dictionnaire contient le théta de la caractéristique respective à la colonne de X\n",
    "# On ajoute les probabilités antérieures des classes à la fin de résultat\n",
    "def entrainer_multi(X, Y): \n",
    "    resultat = []\n",
    "    for i in range(X.shape[1]): \n",
    "        resultat.append(entrainer_multi_1(X[:, i], Y))\n",
    "    resultat.append(P_c(Y))\n",
    "    return resultat\n",
    "\n",
    "Theta_jouer = entrainer_multi(X_jouer, Y_jouer)\n",
    "\n",
    "Theta_jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-4- Estimation de la probabilité de vraissemblance (loi multinomiale)\n",
    "L'équation pour estimer la vraisemblance \n",
    "$$ P(f_j=v|y=c_k) = \\frac{|\\{ y \\in Y / y = c_k \\text{ et } f_j = v\\}|}{|\\{y = c_k\\}|}$$\n",
    "\n",
    "Si, dans le dataset de test, on veut calculer la probabilité d'une valeur $v$ qui n'existe pas dans le dataset d'entrainnement ou qui n'existe pas pour une classe donnée, on aura une probabilité nulle. Ici, on doit appliquer une fonction de lissage qui donne une petite probabilité aux données non vues dans l'entrainnement. Le lissage qu'on va utiliser est celui de Lidstone. Lorsque $\\alpha = 1$ on l'appelle lissage de Laplace.\n",
    "$$ P(f_j=v|y=c_k) = \\frac{|\\{ y \\in Y / y = c_k \\text{ et } f_j = v\\}| + \\alpha}{|\\{y = c_k\\}| + \\alpha * |V|}$$\n",
    "Où: \n",
    "- $\\alpha$ est une valeur donnée \n",
    "- $V$ est l'ensemble des différentes valeurs de $f_j$ (le vocabulaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 0.08333333333333333)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter cette fonction\n",
    "def P_vraiss_multi(Theta_j, v, c, alpha=1.): \n",
    "    len_V = len(Theta_j) # La taille du vocabulaire\n",
    "    nbr_c = 0\n",
    "    for i in Theta_j.keys() :\n",
    "        nbr_c += Theta_j[i][c]\n",
    "    li_V = Theta_j.get(v)\n",
    "    len_V2 = li_V[c] if li_V else 0\n",
    "        \n",
    "    return (len_V2 + alpha) / (nbr_c + alpha * len_V)\n",
    "\n",
    "# La probabilité de jouer si temps = pluvieux \n",
    "# P(temps = pluvieux | jouer=oui) = (nbr(temps=pluvieux et jouer=oui)+alpha)/(nbr(jour=oui) + alpha * nbr_diff(temps)))\n",
    "# P(temps = pluvieux | jouer=oui) = (3 + 1)/(9 + 3) ==> 3 est le nombre de différentes valeurs de temps (entrainnement)\n",
    "# P(temps = pluvieux | jouer=oui) = 4/12 ==> 0.33333333333333333333333333333333333~\n",
    "\n",
    "# La probabilité de jouer si temps = neigeux \n",
    "# P(temps = neigeux | jouer=oui) = (nbr(temps=neigeux et jouer=oui)+alpha)/(nbr(jouer=oui) + alpha * nbr_diff(temps)))\n",
    "# P(temps = neigeux | jouer=oui) = (0 + 1)/(9 + 3) ==> 3 est le nombre de différentes valeurs de temps (entrainnement)\n",
    "# P(temps = neigeux | jouer=oui) = 1/13 ==> 0.0833333333333333333333333333333333333~\n",
    "\n",
    "\n",
    "P_vraiss_multi(Theta_jouer_temps, \"pluvieux\", \"oui\"), P_vraiss_multi(Theta_jouer_temps, \"neigeux\", \"oui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-5- Prédiction de la classe (loi multinomiale)\n",
    "Revenons maintenant à notre équation de prédiction \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j|y=c_k)$$\n",
    "\n",
    "Ici, vous devez prédire un seule échantillon $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('oui', -4.102643365036796), ('oui', -3.6608106127577567))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter ce code\n",
    "# Pour récupérer le théta de la caractéristique n°0 : Theta[0]\n",
    "# anter est un booléen, si il est False, on ne compte pas la probabilité antérieure P(y = c_k)\n",
    "def predire(x, Theta, alpha=1., anter=True): \n",
    "    c_opt = \"\" # la classe optimale\n",
    "    p_c = Theta[-1] #les classes et leurs probabilités antérieures\n",
    "    if not anter: # si on ne veut pas ajouter les probabiliés antérieures\n",
    "        p_c = dict.fromkeys(p_c, 1.) # on définit le tous en 1; log(1) = 0\n",
    "    max_log_p = np.NINF # - infinity \n",
    "    # compléter ici\n",
    "    for p_c_value in p_c.keys():\n",
    "        mmax = np.log(p_c[p_c_value])\n",
    "        i = 0\n",
    "        for th in Theta[:len(Theta) - 1]:\n",
    "            mmax += np.log(P_vraiss_multi(th,x[i],p_c_value))\n",
    "            i += 1\n",
    "        if max_log_p < mmax:\n",
    "            max_log_p = mmax\n",
    "            c_opt = p_c_value\n",
    "    return c_opt, max_log_p\n",
    "\n",
    "# Résultat: (('oui', -4.102643365036796), ('oui', -3.6608106127577567))\n",
    "predire([\"pluvieux\", \"fraiche\", \"normale\", \"oui\"], Theta_jouer), predire([\"pluvieux\", \"fraiche\", \"normale\", \"oui\"], Theta_jouer, anter=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-7- Regrouper en une classe (loi multinomiale)\n",
    "\n",
    "**Rien à programmer ici, il y a une petite analyse**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBMultinom(object): \n",
    "    \n",
    "    def __init__(self, alpha=1.): \n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def entrainer(self, X, Y):\n",
    "        self.Theta = entrainer_multi(X, Y)\n",
    "    \n",
    "    def predire(self, X, anter=True, prob=False): \n",
    "        Y_pred = []\n",
    "        for i in range(len(X)): \n",
    "            c, p = predire(X[i,:], self.Theta, alpha=self.alpha, anter=anter)\n",
    "            if prob:\n",
    "                Y_pred.append(p)\n",
    "            else:\n",
    "                Y_pred.append(c)\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va entrainer un modèle en utilisant notre imlémentation avec et sans probabilité antérieure. \n",
    "Normalement, on doit tester sur des données non vues (des données qu'on n'a pas utilisé pour l'entrainement). Mais, ici, on va tester sur les mêmes données d'entrainement afin de savoir si le modèle a bien représenté ce dataset ou non (calculer l'erreur) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notre modèle avec probabilité antérieure (a priori)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         non       0.80      1.00      0.89         4\n",
      "         oui       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.90      0.95      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n",
      "Notre modèle sans probabilité antérieure (a priori)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         non       0.80      0.67      0.73         6\n",
      "         oui       0.78      0.88      0.82         8\n",
      "\n",
      "    accuracy                           0.79        14\n",
      "   macro avg       0.79      0.77      0.78        14\n",
      "weighted avg       0.79      0.79      0.78        14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "notre_modele = NBMultinom()\n",
    "notre_modele.entrainer(X_jouer, Y_jouer)\n",
    "Y_notre_ant = notre_modele.predire(X_jouer)\n",
    "Y_notre_sans_ant = notre_modele.predire(X_jouer, anter=False)\n",
    "\n",
    "# Ici, ce n'ai pas la peine d'exécuter plusieurs fois\n",
    "# puisque le résultat sera le même \n",
    "\n",
    "# Le rapport de classification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Notre modèle avec probabilité antérieure (a priori)\")\n",
    "print(classification_report(Y_notre_ant, Y_jouer))\n",
    "\n",
    "print(\"Notre modèle sans probabilité antérieure (a priori)\")\n",
    "print(classification_report(Y_notre_sans_ant, Y_jouer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser les résultats** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse**\n",
    "- L'entrainement avec la probabilité antérieure:\n",
    "    - Ce modele est plus performant dans la representation des donnes d'entrainementdonne et il donne une precision superieur a 90%\n",
    "    - Il permet de prendre en considération les informations collecter lors du calcul de sa probabilité postérieure\n",
    "    - Ce modele utilise la probabilité antérieure qui est considéré comme une information subjective\n",
    "- L'entrainement sans la probabilité antérieure:\n",
    "    - Ce modele ne tenir pas des sérieux défauts dans sa classification avec une précision de 79%\n",
    "    - Ce module dans son entrainement considere que les classes sont équiprobables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- Détection de spam \n",
    "\n",
    "Ici, on va essayer d'appliquer l'apprentissage automatique sur la détection de spam. \n",
    "Chaque message dans le dataset est représenté en utilisant un modèle \"Sac à mots\" (BoW : Bag of Words).\n",
    "Dans l'entrainement, on récupère les différents mots qui s'apparaissent dans les messages. \n",
    "Chaque mot va être considéré comme une caractéristique. \n",
    "Donc, pour chaque message, la valeur de la caractéristique est la fréquence de son mot dans le message. \n",
    "Par exemple, si le mot \"good\" apparait 3 fois dans le message, donc la caractéristique \"good\" aura la valeur 3 dans ce message.\n",
    "\n",
    "Notre implémentation n'est pas adéquate pour la nature de ce problème. \n",
    "Dans Scikit-learn, le [sklearn.naive_bayes.CategoricalNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html) est similaire à notre implémentation. \n",
    "L'algorithme adéquat pour ce type de problème est [sklearn.naive_bayes.MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).\n",
    "\n",
    "Le dataset utilisé est [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset).\n",
    "Les algorithmes comparés :\n",
    "- Naive Bayes\n",
    "- Arbre de décision\n",
    "- Regression logistique \n",
    "\n",
    "### II-1- Préparation de données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texte</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texte classe\n",
       "0  Go until jurong point, crazy.. Available only ...    ham\n",
       "1                      Ok lar... Joking wif u oni...    ham\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   spam\n",
       "3  U dun say so early hor... U c already then say...    ham\n",
       "4  Nah I don't think he goes to usf, he lives aro...    ham"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = pd.read_csv(\"datasets/spam.csv\", encoding=\"latin-1\")\n",
    "messages = messages.rename(columns={\"v1\": \"classe\", \"v2\": \"texte\"})\n",
    "messages = messages.filter([\"texte\", \"classe\"])\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-2- Entrainement et test des modèles sur plusieurs exécutions \n",
    "\n",
    "Afin de satisfaire un étudiant qui réclame toujours sur le manque des données, nous avons décidé de comparer les algorithmes sur plusieurs excécutions (runs). \n",
    "\n",
    "**Rien à analyser ici**\n",
    "\n",
    "**P.S.** timeit.default_timer() est dépendante du système d'exploitation. Aussi, elle peut être affectée par d'autre processus en parallèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'naive_bayes': [0.04293207499995333,\n",
       "  0.058731005000026926,\n",
       "  0.038451964999921984,\n",
       "  0.0391760699999395,\n",
       "  0.039536839999982476,\n",
       "  0.03817689800007429,\n",
       "  0.043203037000012046],\n",
       " 'arbre_decision': [0.6312684969998372,\n",
       "  0.6492299910000838,\n",
       "  0.6779052589999992,\n",
       "  0.5931660109999939,\n",
       "  0.5796702569998615,\n",
       "  0.5736747110001943,\n",
       "  0.5200699170000007],\n",
       " 'reg_log': [0.40534267199996066,\n",
       "  0.34928382399994007,\n",
       "  0.28152331899991623,\n",
       "  0.32211629499988703,\n",
       "  0.3667008279999209,\n",
       "  0.25701614399986283,\n",
       "  0.29666485799998554]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import timeit\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "NBR_RUN = 7\n",
    "\n",
    "temps_train = {\n",
    "    \"naive_bayes\" : [],\n",
    "    \"arbre_decision\": [],\n",
    "    \"reg_log\": []\n",
    "}\n",
    "\n",
    "temps_test = {\n",
    "    \"naive_bayes\" : [],\n",
    "    \"arbre_decision\": [],\n",
    "    \"reg_log\": []\n",
    "}\n",
    "\n",
    "perf = {\n",
    "    \"naive_bayes_P\" : [],\n",
    "    \"arbre_decision_P\": [],\n",
    "    \"reg_log_P\": [], \n",
    "    \"naive_bayes_R\" : [],\n",
    "    \"arbre_decision_R\": [],\n",
    "    \"reg_log_R\": []\n",
    "}\n",
    "\n",
    "\n",
    "for run in range(NBR_RUN): \n",
    "    # prétaitement des données\n",
    "    msg_train, msg_test, Y_train, Y_test = train_test_split(messages[\"texte\"],messages[\"classe\"],test_size=0.2)\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    X_train = count_vectorizer.fit_transform(msg_train)\n",
    "    X_test = count_vectorizer.transform(msg_test)\n",
    "    \n",
    "    # ==================================\n",
    "    # ENTRAINEMENT \n",
    "    # ==================================\n",
    "    \n",
    "    #entrainement Naive Bayes\n",
    "    naive_bayes = MultinomialNB()\n",
    "    temps_debut = timeit.default_timer()\n",
    "    naive_bayes.fit(X_train, Y_train)\n",
    "    temps_train[\"naive_bayes\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    #entrainement CART\n",
    "    arbre_decision = DecisionTreeClassifier()\n",
    "    temps_debut = timeit.default_timer()\n",
    "    arbre_decision.fit(X_train, Y_train)\n",
    "    temps_train[\"arbre_decision\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    #entrainement Régression logitique\n",
    "    reg_log = LogisticRegression(solver=\"lbfgs\") #solver=sag est plus lent; donc j'ai choisi le plus rapide\n",
    "    temps_debut = timeit.default_timer()\n",
    "    reg_log.fit(X_train, Y_train)\n",
    "    temps_train[\"reg_log\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    # ==================================\n",
    "    # TEST \n",
    "    # ==================================\n",
    "    \n",
    "    #test Naive Bayes\n",
    "    temps_debut = timeit.default_timer()\n",
    "    Y_naive_bayes = naive_bayes.predict(X_test)\n",
    "    temps_test[\"naive_bayes\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    \n",
    "    #test CART\n",
    "    temps_debut = timeit.default_timer()\n",
    "    Y_arbre_decision = arbre_decision.predict(X_test)\n",
    "    temps_test[\"arbre_decision\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    #test Régression logitique\n",
    "    temps_debut = timeit.default_timer()\n",
    "    Y_reg_log = reg_log.predict(X_test)\n",
    "    temps_test[\"reg_log\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    # ==================================\n",
    "    # PERFORMANCE \n",
    "    # ==================================\n",
    "    # Ici, on va considérer une classification binaire avec une seule classe \"spam\" \n",
    "    # On ne juge pas le classifieur sur sa capacité de détecter les non spams\n",
    "    \n",
    "    perf[\"naive_bayes_P\"].append(precision_score(Y_test, Y_naive_bayes, pos_label=\"spam\"))\n",
    "    perf[\"arbre_decision_P\"].append(precision_score(Y_test, Y_arbre_decision, pos_label=\"spam\"))\n",
    "    perf[\"reg_log_P\"].append(precision_score(Y_test, Y_reg_log, pos_label=\"spam\"))\n",
    "    \n",
    "    perf[\"naive_bayes_R\"].append(recall_score(Y_test, Y_naive_bayes, pos_label=\"spam\"))\n",
    "    perf[\"arbre_decision_R\"].append(recall_score(Y_test, Y_arbre_decision, pos_label=\"spam\"))\n",
    "    perf[\"reg_log_R\"].append(recall_score(Y_test, Y_reg_log, pos_label=\"spam\"))\n",
    "    \n",
    "    \n",
    "\n",
    "temps_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-3- Analyse du temps d'apprentissage \n",
    "\n",
    "Combien de temps chaque algorithme prend pour entrainer le même dataset d'entrainement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>naive_bayes</th>\n",
       "      <th>arbre_decision</th>\n",
       "      <th>reg_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.042932</td>\n",
       "      <td>0.631268</td>\n",
       "      <td>0.405343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.058731</td>\n",
       "      <td>0.649230</td>\n",
       "      <td>0.349284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.038452</td>\n",
       "      <td>0.677905</td>\n",
       "      <td>0.281523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.039176</td>\n",
       "      <td>0.593166</td>\n",
       "      <td>0.322116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.039537</td>\n",
       "      <td>0.579670</td>\n",
       "      <td>0.366701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.038177</td>\n",
       "      <td>0.573675</td>\n",
       "      <td>0.257016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>0.520070</td>\n",
       "      <td>0.296665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   naive_bayes  arbre_decision   reg_log\n",
       "0     0.042932        0.631268  0.405343\n",
       "1     0.058731        0.649230  0.349284\n",
       "2     0.038452        0.677905  0.281523\n",
       "3     0.039176        0.593166  0.322116\n",
       "4     0.039537        0.579670  0.366701\n",
       "5     0.038177        0.573675  0.257016\n",
       "6     0.043203        0.520070  0.296665"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(temps_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse**\n",
    "- La methode naive_bayse est la plus rapide des trois methodes dans la construction du modele grace a sa simplicité (naive)\n",
    "- L'arbre de decision est la plus lente a cause de la construction incrémentale de l'arbre qui prend beaucoup de temp\n",
    "- La methode de regression logistique est au milieu de ces deux methode avec un temp acceptable dans l'entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-4- Analyse du temps de test \n",
    "\n",
    "Combien de temps chaque algorithme prend pour prédir les classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>naive_bayes</th>\n",
       "      <th>arbre_decision</th>\n",
       "      <th>reg_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.002706</td>\n",
       "      <td>0.000779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.000447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.001780</td>\n",
       "      <td>0.000591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.000725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.000435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>0.000451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.000451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   naive_bayes  arbre_decision   reg_log\n",
       "0     0.001470        0.002706  0.000779\n",
       "1     0.000879        0.001734  0.000447\n",
       "2     0.000888        0.001780  0.000591\n",
       "3     0.001493        0.002458  0.000725\n",
       "4     0.000858        0.001722  0.000435\n",
       "5     0.000886        0.001730  0.000451\n",
       "6     0.000905        0.001689  0.000451"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(temps_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse**\n",
    "- Les methodes naive_bayse et reg_log prennent moins de temps pour la predection par rapport au temps de l'entrainement\n",
    "- Les methodes naive_bayse et reg_log ont a peu pres le meme temps du test (prediction) (la reg est plus rapide avec une ptite ecart)\n",
    "- L'arbre de decision reste la plus lente des trois methodes dans le test car il fait un parcoure de tout l'arbre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-5- Analyse de la performance \n",
    "\n",
    "Ici, on compare les modèles en se basant sur leurs capacités à détecter le spam. \n",
    "On va utiliser la précision et le rappel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arbre_decision_P</th>\n",
       "      <th>naive_bayes_P</th>\n",
       "      <th>reg_log_P</th>\n",
       "      <th>arbre_decision_R</th>\n",
       "      <th>naive_bayes_R</th>\n",
       "      <th>reg_log_R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.860140</td>\n",
       "      <td>0.963504</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>0.929577</td>\n",
       "      <td>0.859155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.855172</td>\n",
       "      <td>0.992126</td>\n",
       "      <td>0.961832</td>\n",
       "      <td>0.843537</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.874172</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.965986</td>\n",
       "      <td>0.959184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.908451</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.915033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.896970</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.987342</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.939759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.938356</td>\n",
       "      <td>0.980132</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.867089</td>\n",
       "      <td>0.936709</td>\n",
       "      <td>0.892405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.987097</td>\n",
       "      <td>0.985816</td>\n",
       "      <td>0.888199</td>\n",
       "      <td>0.950311</td>\n",
       "      <td>0.863354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   arbre_decision_P  naive_bayes_P  reg_log_P  arbre_decision_R  \\\n",
       "0          0.860140       0.963504   0.976000          0.866197   \n",
       "1          0.855172       0.992126   0.961832          0.843537   \n",
       "2          0.874172       0.959459   0.979167          0.897959   \n",
       "3          0.908451       0.985915   0.979021          0.843137   \n",
       "4          0.896970       0.941176   0.987342          0.891566   \n",
       "5          0.938356       0.980132   0.972414          0.867089   \n",
       "6          0.928571       0.987097   0.985816          0.888199   \n",
       "\n",
       "   naive_bayes_R  reg_log_R  \n",
       "0       0.929577   0.859155  \n",
       "1       0.857143   0.857143  \n",
       "2       0.965986   0.959184  \n",
       "3       0.915033   0.915033  \n",
       "4       0.963855   0.939759  \n",
       "5       0.936709   0.892405  \n",
       "6       0.950311   0.863354  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(perf, columns = [\"arbre_decision_P\", \"naive_bayes_P\", \"reg_log_P\", \"arbre_decision_R\", \"naive_bayes_R\", \"reg_log_R\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse**\n",
    "- **La methode naive_bayse:**\n",
    "\n",
    "    - Il ne comporte aucun surapprentissage\n",
    "    - Il est capable de traiter un très grand nombre de caractéristiques\n",
    "    - Il est le meilleure modele avec plus de 90% des message classifié correctement comme SPAM donc avec une precision de 95% .\n",
    "- **La methode de regression logistique:**\n",
    "\n",
    "    - C'est le modele le plus proches de celui de la methodes naive_bays dans la performance\n",
    "    - Il a plus de probabilte pour avoir un surapprentissage\n",
    "    - Il predire 86% des email dit SPAM correcctement avec un precision de 96%\n",
    "    \n",
    "- **l'arbre de decision:**\n",
    "\n",
    "    - Il a le risque d'avoir un surapprentissage\n",
    "    - Présente un modele qui peut predire 84% des email dit SPAM correcctement avec un precision de 90%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
